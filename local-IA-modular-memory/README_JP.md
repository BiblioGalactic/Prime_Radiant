ローカルモジュール型 AI メモリ

ローカルモジュール型 AI メモリは、Bashで作られた公開スクリプトで、Markdownノートから完全な LLaMA プロンプトを生成・実行します。指定ディレクトリ内の .md ファイルをすべて結合し、不要な空白や空行を削除した後、LLaMA のインタラクティブセッションを起動します。

⸻

機能
	•	任意の .md ファイルディレクトリに対応
	•	UTF-8 を維持しつつ、空白や空行を整理
	•	モデルと llama-cli 実行ファイルのパスを入力
	•	プロンプト生成前に動的更新（オプション）

⸻

使用方法

./local_ia_modular_memory.sh

画面の指示に従って操作：
	1.	.md ファイルが含まれるディレクトリを入力
	2.	LLaMA モデルのパスを入力（.gguf ファイル）
	3.	llama-cli 実行ファイルのパスを入力

スクリプトは prompt_completo.txt を生成し、LLaMA のインタラクティブセッションを起動します。

⸻

システム要件
	•	Bash >= 5
	•	LLaMA CLI がインストール済み
	•	ローカル GGUF モデル

⸻

