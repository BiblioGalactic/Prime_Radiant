Version en FranÃ§ais

MÃ©moire IA Modulaire Locale est un script public en Bash permettant de gÃ©nÃ©rer et dâ€™exÃ©cuter un prompt complet avec LLaMA Ã  partir de vos notes en Markdown.
Il concatÃ¨ne tous les fichiers .md dâ€™un rÃ©pertoire, les nettoie et lance une session interactive de LLaMA.

â¸»

ğŸ§  FonctionnalitÃ©s
	â€¢	Fonctionne avec nâ€™importe quel rÃ©pertoire contenant des fichiers .md.
	â€¢	Nettoie les espaces et lignes vides tout en conservant lâ€™encodage UTF-8.
	â€¢	Demande les chemins du modÃ¨le et de lâ€™exÃ©cutable llama-cli.
	â€¢	Mise Ã  jour dynamique optionnelle avant la gÃ©nÃ©ration du prompt.

â¸»

âš™ï¸ Utilisation

./local_ia_modular_memory.sh

Suivez les instructions :
	1.	Indiquez le rÃ©pertoire contenant vos fichiers .md.
	2.	Indiquez le chemin vers votre modÃ¨le LLaMA (.gguf).
	3.	Indiquez le chemin vers lâ€™exÃ©cutable llama-cli.

Le script gÃ©nÃ©rera un fichier prompt_completo.txt et lancera une session interactive de LLaMA.

â¸»

ğŸ§© PrÃ©requis
	â€¢	Bash >= 5
	â€¢	LLaMA CLI installÃ©
	â€¢	ModÃ¨le local .gguf

â¸»

ğŸ“„ Licence

Open-source. Vous pouvez lâ€™utiliser librement, le modifier et le partager.

â¸»

Eto Demerzel (Gustavo Silva Da Costa)
ğŸ”— etodemerzel.gumroad.com
ğŸ”— github.com/BiblioGalactic