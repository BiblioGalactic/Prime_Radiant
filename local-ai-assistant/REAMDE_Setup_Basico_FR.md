ðŸ¤– Configuration de lâ€™Assistant IA Local - Installateur de Base

Description

Script dâ€™installation automatisÃ©e pour configurer un assistant IA local utilisant les modÃ¨les llama.cpp. Cet installateur est conÃ§u pour Ãªtre simple, direct et facile Ã  utiliser, fournissant une base solide pour interagir avec des modÃ¨les de langage locaux.

CaractÃ©ristiques principales

ðŸ”§ Configuration simple et intuitive
	â€¢	Installation guidÃ©e : configuration interactive Ã©tape par Ã©tape
	â€¢	Validation automatique : vÃ©rifie les prÃ©requis et les chemins
	â€¢	Configuration adaptative : sâ€™ajuste Ã  diffÃ©rents environnements
	â€¢	Structure modulaire : architecture organisÃ©e et extensible

ðŸŽ¯ FonctionnalitÃ©s principales
	â€¢	Client LLM : communication directe avec llama.cpp
	â€¢	Gestionnaire de fichiers : opÃ©rations sÃ»res de lecture/Ã©criture
	â€¢	ExÃ©cuteur de commandes : exÃ©cution contrÃ´lÃ©e du systÃ¨me
	â€¢	Configuration flexible : JSON configurable

ðŸ“ Architecture modulaire

src/
â”œâ”€â”€ core/           # Moteur principal de lâ€™assistant
â”œâ”€â”€ llm/            # Client pour llama.cpp
â”œâ”€â”€ file_ops/       # Gestion des fichiers
â””â”€â”€ commands/       # ExÃ©cution des commandes

PrÃ©requis systÃ¨me
	â€¢	Python 3.11+
	â€¢	llama.cpp compilÃ©
	â€¢	ModÃ¨le GGUF compatible
	â€¢	pip3 pour les dÃ©pendances Python
	â€¢	SystÃ¨me dâ€™exploitation : macOS, Linux

Installation rapide

1. TÃ©lÃ©chargement et exÃ©cution

# TÃ©lÃ©charger le script
curl -O https://raw.githubusercontent.com/ton-utilisateur/asistente-basico/main/setup_asistente_basico.sh

# Rendre exÃ©cutable
chmod +x setup_asistente_basico.sh

# Lancer lâ€™installation
./setup_asistente_basico.sh

2. Configuration interactive

Le script vous demandera :

RÃ©pertoire du projet :

RÃ©pertoire du projet [/Users/ton-utilisateur/assistant-ia]: 

Chemin du modÃ¨le GGUF :

Chemin du modÃ¨le GGUF [/Users/ton-utilisateur/modele/modele.gguf]: 

Chemin de llama-cli :

Chemin de llama.cpp [/Users/ton-utilisateur/llama.cpp/build/bin/llama-cli]: 

3. Confirmation

Configuration sÃ©lectionnÃ©e :
RÃ©pertoire du projet : /Users/ton-utilisateur/assistant-ia
ModÃ¨le : /Users/ton-utilisateur/modele/modele.gguf
Llama.cpp : /Users/ton-utilisateur/llama.cpp/build/bin/llama-cli

Continuer avec cette configuration ? (y/N)

Structure gÃ©nÃ©rÃ©e

assistant-ia/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Point dâ€™entrÃ©e principal
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ assistant.py        # Classe principale de lâ€™assistant
â”‚   â”‚   â””â”€â”€ config.py           # Gestion de la configuration
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ client.py           # Client llama.cpp
â”‚   â”œâ”€â”€ file_ops/
â”‚   â”‚   â””â”€â”€ manager.py          # Gestion des fichiers
â”‚   â””â”€â”€ commands/
â”‚       â””â”€â”€ runner.py           # ExÃ©cution des commandes
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.json           # Configuration principale
â”œâ”€â”€ tools/                      # Outils additionnels
â”œâ”€â”€ tests/                      # Tests systÃ¨me
â”œâ”€â”€ logs/                       # Fichiers journaux
â””â”€â”€ examples/                   # Exemples dâ€™utilisation

Utilisation basique

Commande principale

cd /chemin/vers/ton/assistant-ia
python3 src/main.py "Quels fichiers Python se trouvent dans ce projet ?"

Mode interactif

python3 src/main.py
ðŸ¤– Assistant IA Local - Mode interactif
Tapez 'exit' pour quitter, 'help' pour lâ€™aide

ðŸ’¬ > explique le fichier main.py
ðŸ¤– Le fichier main.py est le point dâ€™entrÃ©e...

ðŸ’¬ > exit
Ã€ bientÃ´t ! ðŸ‘‹

ParamÃ¨tres en ligne de commande

# Utiliser une configuration spÃ©cifique
python3 src/main.py --config config/custom.json "analyse ce projet"

# Mode verbeux
python3 src/main.py --verbose "liste les fichiers Python"

# Aide
python3 src/main.py --help

Configuration

Fichier de configuration (config/settings.json)

{
  "llm": {
    "model_path": "/chemin/vers/ton/modele.gguf",
    "llama_bin": "/chemin/vers/llama-cli",
    "max_tokens": 1024,
    "temperature": 0.7
  },
  "assistant": {
    "safe_mode": true,
    "backup_files": true,
    "max_file_size": 1048576,
    "supported_extensions": [".py", ".js", ".ts", ".json", ".md", ".txt", ".sh"]
  },
  "logging": {
    "level": "INFO",
    "file": "logs/assistant.log"
  }
}