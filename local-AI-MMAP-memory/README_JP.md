ローカル AI MMAP メモリ

Local AI MMAP Memory は、Bash + C で作られたパブリックランチャーで、mmap を使って複数のモジュール化プロファイルを直接メモリにロードし、LLaMA を実行できます。各プロファイルは異なる AI コンテキスト（技術的、哲学的、安全など）を表し、一時ファイルなしで効率的にプロンプトを管理可能です。

⸻

機能
	•	複数の .txt プロファイルをメモリにロード
	•	実行時にアクティブプロファイルを選択
	•	mmap 経由でロードされたコンテキストで LLaMA を対話的に実行
	•	ポータブル＆オープンソース：ユーザーは独自のパスを指定可能
	•	ファイル、mmap、LLaMA 起動のエラー処理

⸻

使い方

./local-AI-MMAP-memory.sh

手順に従って：
	1.	プロンプトファイル (.txt) を入力
	2.	llama-cli 実行ファイルのパスを入力
	3.	.gguf モデルのパスを入力
	4.	プロファイルのパスをカンマで区切って入力
	5.	アクティブプロファイルのインデックスを選択

⸻

必要条件
	•	Bash >= 5
	•	GCC
	•	LLaMA CLI インストール済み
	•	ローカル .gguf モデル

⸻
