ü§ñ Configuration Assistant IA Local Am√©lior√©

Description

Syst√®me d‚Äôinstallation automatis√© pour un Assistant IA Local avec capacit√©s agentiques avanc√©es. Ce script configure un environnement de d√©veloppement complet pour interagir avec des mod√®les LLaMA locaux, fournissant des fonctionnalit√©s d‚Äôanalyse de code, gestion de fichiers et ex√©cution de commandes syst√®me.

Principales fonctionnalit√©s

üß† Mode agentique intelligent
	‚Ä¢	Planification automatique : d√©compose les t√¢ches complexes en sous-t√¢ches sp√©cifiques
	‚Ä¢	Lecture automatique des fichiers : analyse automatiquement les fichiers pertinents du projet
	‚Ä¢	Synth√®se sans redondances : combine plusieurs analyses en √©liminant les informations r√©p√©t√©es
	‚Ä¢	V√©rification de la qualit√© : syst√®me automatique de contr√¥le qualit√© des r√©ponses

üîß Fonctionnalit√©s avanc√©es
	‚Ä¢	50+ commandes activ√©es : Git, Docker, NPM, Python, et plus
	‚Ä¢	Protection contre commandes dangereuses : syst√®me de s√©curit√© int√©gr√©
	‚Ä¢	Gestion intelligente des fichiers : lecture, √©criture et analyse de code
	‚Ä¢	Configuration adaptative : s‚Äôajuste automatiquement √† l‚Äôenvironnement

üéØ Architecture modulaire
	‚Ä¢	Core : moteur principal de l‚Äôassistant
	‚Ä¢	LLM Client : communication avec les mod√®les llama.cpp
	‚Ä¢	File Manager : gestion s√©curis√©e des fichiers
	‚Ä¢	Command Runner : ex√©cution contr√¥l√©e des commandes
	‚Ä¢	Agentic Extension : capacit√©s agentiques avanc√©es

Pr√©requis syst√®me
	‚Ä¢	Python 3.11+
	‚Ä¢	llama.cpp compil√© et fonctionnel
	‚Ä¢	Mod√®le GGUF compatible
	‚Ä¢	Bash 4.0+
	‚Ä¢	Syst√®me d‚Äôexploitation : macOS, Linux

Installation

1. T√©l√©chargement et installation

# T√©l√©charger le script
curl -O https://raw.githubusercontent.com/tu-usuario/setup-asistente/main/setup_asistente.sh

# Rendre ex√©cutable
chmod +x setup_asistente.sh

# Ex√©cuter l'installation
./setup_asistente.sh

2. Configuration interactive

Le script vous demandera :
	‚Ä¢	R√©pertoire d‚Äôinstallation : o√π le projet sera install√©
	‚Ä¢	Chemin du mod√®le GGUF : votre mod√®le de langage local
	‚Ä¢	Chemin de llama-cli : binaire de llama.cpp

3. Structure g√©n√©r√©e

asistente-ia/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/              # Moteur principal
‚îÇ   ‚îú‚îÄ‚îÄ llm/               # Client LLM
‚îÇ   ‚îú‚îÄ‚îÄ file_ops/          # Gestion des fichiers
‚îÇ   ‚îî‚îÄ‚îÄ commands/          # Ex√©cution des commandes
‚îú‚îÄ‚îÄ config/                # Configuration
‚îú‚îÄ‚îÄ tools/                 # Outils additionnels
‚îú‚îÄ‚îÄ tests/                 # Tests syst√®me
‚îú‚îÄ‚îÄ logs/                  # Journaux d'ex√©cution
‚îî‚îÄ‚îÄ examples/              # Exemples d'utilisation

Utilisation

Commandes de base

# Assistant normal
claudia "explique ce projet"

# Mode agentique
claudia-a "analyse compl√®tement l'architecture"

# Mode verbose (voir le processus interne)
claudia-deep "investigation approfondie des erreurs"

# Aide compl√®te
claudia-help

Exemples de commandes agentiques
	‚Ä¢	"analyse compl√®tement la structure du code"
	‚Ä¢	"investigation approfondie sur la performance"
	‚Ä¢	"mode agentique : optimise tout le code"
	‚Ä¢	"examine en d√©tail les erreurs"

Mode interactif

claudia
üí¨ > agentic on
üí¨ > analyse compl√®tement ce projet
üí¨ > exit

Configuration avanc√©e

Fichier de configuration

{
  "llm": {
    "model_path": "/chemin/vers/ton/modele.gguf",
    "llama_bin": "/chemin/vers/llama-cli",
    "max_tokens": 1024,
    "temperature": 0.7
  },
  "assistant": {
    "safe_mode": false,
    "backup_files": true,
    "supported_extensions": [".py", ".js", ".json", ".md"]
  }
}

Personnalisation
	‚Ä¢	Mod√®les : changez le chemin du mod√®le dans config/settings.json
	‚Ä¢	Commandes : modifiez la liste des commandes autoris√©es dans commands/runner.py
	‚Ä¢	Extensions : ajoutez de nouveaux types de fichiers support√©s

Architecture du syst√®me

Composants principaux
	1.	LocalAssistant : classe principale qui coordonne tous les composants
	2.	AgenticAssistant : extension fournissant les capacit√©s agentiques
	3.	LlamaClient : interface avec les mod√®les llama.cpp
	4.	FileManager : gestion s√©curis√©e des fichiers du projet
	5.	CommandRunner : ex√©cution contr√¥l√©e des commandes syst√®me

Flux agentique
	1.	Planification : d√©compose la t√¢che en sous-t√¢ches sp√©cifiques
	2.	Ex√©cution : ex√©cute chaque sous-t√¢che avec contexte enrichi
	3.	Synth√®se : combine les r√©sultats en √©liminant les redondances
	4.	V√©rification : valide la qualit√© de la r√©ponse finale

S√©curit√©

Commandes prohib√©es
	‚Ä¢	rm, rmdir, dd, shred
	‚Ä¢	sudo, su, chmod, chown
	‚Ä¢	kill, reboot, shutdown

Commandes autoris√©es
	‚Ä¢	Outils de d√©veloppement : git, npm, pip, docker
	‚Ä¢	Analyse de fichiers : cat, grep, find, head, tail
	‚Ä¢	Compilation : make, cmake, gradle, maven

R√©solution de probl√®mes

Erreur : ‚Äúllama-cli introuvable‚Äù

# V√©rifier l'installation de llama.cpp
which llama-cli

# Mettre √† jour le chemin dans la config
vim config/settings.json

Erreur : ‚ÄúMod√®le introuvable‚Äù

# V√©rifier le chemin du mod√®le
ls -la /chemin/vers/ton/modele.gguf

# Mettre √† jour la configuration
claudia --config config/settings.json

Le mode agentique ne fonctionne pas

# V√©rifier en mode verbose
claudia-deep "test simple"

# Consulter les logs
tail -f logs/assistant.log

Contribution
	1.	Forkez le d√©p√¥t
	2.	Cr√©ez une branche pour votre fonctionnalit√© : git checkout -b feature/nouvelle-fonctionnalite
	3.	Committez vos changements : git commit -am 'Ajouter nouvelle fonctionnalit√©'
	4.	Pushez la branche : git push origin feature/nouvelle-fonctionnalite
	5.	Ouvrez une Pull Request

Licence

MIT License - voir le fichier LICENSE pour les d√©tails.

Auteur

Gustavo Silva da Costa (Eto Demerzel)

Version

2.0.0 - Syst√®me agentique am√©lior√© avec planification intelligente et synth√®se sans redondances.

‚∏ª

Pour un support suppl√©mentaire, cr√©ez un issue dans le d√©p√¥t du projet.
