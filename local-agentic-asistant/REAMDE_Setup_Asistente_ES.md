# ü§ñ Setup Asistente IA Local Mejorado

## Descripci√≥n

Sistema de instalaci√≥n automatizada para un Asistente IA Local con capacidades ag√©nticas avanzadas. Este script configura un entorno completo de desarrollo para interactuar con modelos LLaMA locales, proporcionando funcionalidades de an√°lisis de c√≥digo, gesti√≥n de archivos y ejecuci√≥n de comandos del sistema.

## Caracter√≠sticas Principales

### üß† Modo Ag√©ntico Inteligente
- **Planificaci√≥n autom√°tica**: Descompone tareas complejas en subtareas espec√≠ficas
- **Lectura autom√°tica de archivos**: Analiza autom√°ticamente archivos relevantes del proyecto
- **S√≠ntesis sin redundancias**: Combina m√∫ltiples an√°lisis eliminando informaci√≥n repetida
- **Verificaci√≥n de calidad**: Sistema autom√°tico de control de calidad de respuestas

### üîß Funcionalidades Avanzadas
- **50+ comandos habilitados**: Git, Docker, NPM, Python, y m√°s
- **Protecci√≥n contra comandos peligrosos**: Sistema de seguridad integrado
- **Gesti√≥n inteligente de archivos**: Lectura, escritura y an√°lisis de c√≥digo
- **Configuraci√≥n adaptativa**: Se ajusta autom√°ticamente al entorno

### üéØ Arquitectura Modular
- **Core**: Motor principal del asistente
- **LLM Client**: Comunicaci√≥n con modelos llama.cpp
- **File Manager**: Gesti√≥n segura de archivos
- **Command Runner**: Ejecuci√≥n controlada de comandos
- **Agentic Extension**: Capacidades ag√©nticas avanzadas

## Requisitos del Sistema

- **Python 3.11+**
- **llama.cpp** compilado y funcional
- **Modelo GGUF** compatible
- **Bash 4.0+**
- **Sistema operativo**: macOS, Linux

## Instalaci√≥n

### 1. Descarga e Instalaci√≥n
```bash
# Descargar el script
curl -O https://raw.githubusercontent.com/tu-usuario/setup-asistente/main/setup_asistente.sh

# Hacer ejecutable
chmod +x setup_asistente.sh

# Ejecutar instalaci√≥n
./setup_asistente.sh
```

### 2. Configuraci√≥n Interactiva
El script te pedir√°:
- **Directorio de instalaci√≥n**: Donde se instalar√° el proyecto
- **Ruta del modelo GGUF**: Tu modelo de lenguaje local
- **Ruta de llama-cli**: Binario de llama.cpp

### 3. Estructura Generada
```
asistente-ia/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/              # Motor principal
‚îÇ   ‚îú‚îÄ‚îÄ llm/               # Cliente LLM
‚îÇ   ‚îú‚îÄ‚îÄ file_ops/          # Gesti√≥n de archivos
‚îÇ   ‚îî‚îÄ‚îÄ commands/          # Ejecuci√≥n de comandos
‚îú‚îÄ‚îÄ config/                # Configuraci√≥n
‚îú‚îÄ‚îÄ tools/                 # Herramientas adicionales
‚îú‚îÄ‚îÄ tests/                 # Tests del sistema
‚îú‚îÄ‚îÄ logs/                  # Logs de ejecuci√≥n
‚îî‚îÄ‚îÄ examples/              # Ejemplos de uso
```

## Uso

### Comandos B√°sicos
```bash
# Asistente normal
claudia "explica este proyecto"

# Modo ag√©ntico
claudia-a "analiza completamente la arquitectura"

# Modo verbose (ver proceso interno)
claudia-deep "investigaci√≥n profunda sobre errores"

# Ayuda completa
claudia-help
```

### Ejemplos de Comandos Ag√©nticos
- `"analiza completamente la estructura del c√≥digo"`
- `"investigaci√≥n profunda sobre el rendimiento"`
- `"modo ag√©ntico: optimiza todo el c√≥digo"`
- `"examina detalladamente los errores"`

### Modo Interactivo
```bash
claudia
üí¨ > agentic on
üí¨ > analiza completamente este proyecto
üí¨ > exit
```

## Configuraci√≥n Avanzada

### Archivo de Configuraci√≥n
```json
{
  "llm": {
    "model_path": "/ruta/a/tu/modelo.gguf",
    "llama_bin": "/ruta/a/llama-cli",
    "max_tokens": 1024,
    "temperature": 0.7
  },
  "assistant": {
    "safe_mode": false,
    "backup_files": true,
    "supported_extensions": [".py", ".js", ".json", ".md"]
  }
}
```

### Personalizaci√≥n
- **Modelos**: Cambia la ruta del modelo en `config/settings.json`
- **Comandos**: Modifica la lista de comandos permitidos en `commands/runner.py`
- **Extensiones**: A√±ade nuevos tipos de archivo soportados

## Arquitectura del Sistema

### Componentes Principales

1. **LocalAssistant**: Clase principal que coordina todos los componentes
2. **AgenticAssistant**: Extensi√≥n que proporciona capacidades ag√©nticas
3. **LlamaClient**: Interface con modelos llama.cpp
4. **FileManager**: Gesti√≥n segura de archivos del proyecto
5. **CommandRunner**: Ejecuci√≥n controlada de comandos del sistema

### Flujo Ag√©ntico

1. **Planificaci√≥n**: Descompone la tarea en subtareas espec√≠ficas
2. **Ejecuci√≥n**: Ejecuta cada subtarea con contexto enriquecido
3. **S√≠ntesis**: Combina resultados eliminando redundancias
4. **Verificaci√≥n**: Valida la calidad de la respuesta final

## Seguridad

### Comandos Prohibidos
- `rm`, `rmdir`, `dd`, `shred`
- `sudo`, `su`, `chmod`, `chown`
- `kill`, `reboot`, `shutdown`

### Comandos Permitidos
- Herramientas de desarrollo: `git`, `npm`, `pip`, `docker`
- An√°lisis de archivos: `cat`, `grep`, `find`, `head`, `tail`
- Compilaci√≥n: `make`, `cmake`, `gradle`, `maven`

## Soluci√≥n de Problemas

### Error: "llama-cli no encontrado"
```bash
# Verificar instalaci√≥n de llama.cpp
which llama-cli

# Actualizar ruta en config
vim config/settings.json
```

### Error: "Modelo no encontrado"
```bash
# Verificar ruta del modelo
ls -la /ruta/a/tu/modelo.gguf

# Actualizar configuraci√≥n
claudia --config config/settings.json
```

### Modo Ag√©ntico no Funciona
```bash
# Verificar modo verbose
claudia-deep "test simple"

# Ver logs
tail -f logs/assistant.log
```

## Contribuci√≥n

1. Fork del repositorio
2. Crear rama para tu feature: `git checkout -b feature/nueva-funcionalidad`
3. Commit de cambios: `git commit -am 'A√±adir nueva funcionalidad'`
4. Push a la rama: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

## Licencia

MIT License - Ver archivo LICENSE para detalles.

## Autor

**Gustavo Silva da Costa (Eto Demerzel)**

## Versi√≥n

**2.0.0** - Sistema ag√©ntico mejorado con planificaci√≥n inteligente y s√≠ntesis sin redundancias.

---

*Para soporte adicional, crear un issue en el repositorio del proyecto.*
