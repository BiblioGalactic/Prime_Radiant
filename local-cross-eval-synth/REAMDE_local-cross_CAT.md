# ü§ñ Local-CROS: Cross-Referential Optimization System

## Descripci√≥

**Local-CROS** √©s un sistema avan√ßat d'avaluaci√≥ creuada per a models LLaMA locals que permet comparar respostes entre m√∫ltiples models i generar respostes optimitzades mitjan√ßant s√≠ntesi intel¬∑ligent. El sistema implementa un enfocament √∫nic d'avaluaci√≥ m√∫tua on cada model avalua les respostes dels altres.

## Caracter√≠stiques Principals

### üîÑ Avaluaci√≥ Creuada
- **Avaluaci√≥ m√∫tua**: Cada model avalua les respostes de tots els altres
- **M√∫ltiples perspectives**: Obtingues diferents enfocaments per a la mateixa pregunta
- **Puntuaci√≥ autom√†tica**: Sistema de scoring autom√†tic per a cada resposta
- **Historial complet**: Registre detallat de totes les interaccions

### üéØ S√≠ntesi Intel¬∑ligent
- **Detecci√≥ autom√†tica del tipus de contingut**: Codi, llistes, poesia, di√†legs, etc.
- **Combinaci√≥ optimitzada**: Fusiona les millors parts de cada resposta
- **Eliminaci√≥ de redund√†ncies**: Evita informaci√≥ repetida
- **Recomanacions contextuals**: Suggeriments espec√≠fics segons el tipus de contingut

### üìä Sistema d'Arxius Incrementals
- **Numeraci√≥ autom√†tica**: `modelo1.txt`, `modelo2.txt`, etc.
- **Historial acumulatiu**: Totes les execucions en un arxiu central
- **Timestamps detallats**: Registre temporal de cada operaci√≥
- **Tra√ßabilitat completa**: Seguiment de tota l'evoluci√≥

## Requisits del Sistema

- **llama.cpp** compilat i funcional
- **2-4 models GGUF** compatibles
- **Bash 4.0+**
- **Eines b√†siques**: `find`, `sed`, `sort`, `jq` (opcional)
- **Sistema operatiu**: macOS, Linux

## Instal¬∑laci√≥

### 1. Desc√†rrega
```bash
# Clonar repositori
git clone https://github.com/tu-usuario/local-cros.git
cd local-cros

# Fer executable
chmod +x local-cros.sh
```

### 2. Primera Configuraci√≥
```bash
# Executar per primera vegada (configuraci√≥ interactiva)
./local-cros.sh
```

L'script et demanar√†:
- **Ruta de llama-cli**: Ubicaci√≥ del binari llama.cpp
- **Directori de treball**: On desar resultats
- **Configuraci√≥ de models**: Nom i ruta de cada model (2-4 models)

### 3. Arxiu de Configuraci√≥ Generat
```bash
# local-cros.conf
LLAMA_CLI_PATH="/path/to/llama-cli"
WORK_DIR="./results"

MODEL_1_NAME="mistral"
MODEL_1_PATH="/path/to/mistral.gguf"

MODEL_2_NAME="llama"
MODEL_2_PATH="/path/to/llama.gguf"
# ... etc
```

## √ös

### Mode Interactiu
```bash
./local-cros.sh
What do you need?
> Escriu un poema √®pic sobre programaci√≥ en Python
```

### Mode Comanda Directa
```bash
./local-cros.sh "Explica les difer√®ncies entre React i Vue.js"
```

### Exemple de Sortida
```
ü§ñ Starting model comparison for: "Explica programaci√≥ funcional"

==> Consulting mistral...
[mistral] says: La programaci√≥ funcional √©s un paradigma...
---

==> Consulting llama...
[llama] says: En programaci√≥ funcional, les funcions s√≥n...
---

=== CROSS-EVALUATION BETWEEN MODELS ===
=== EVALUATION WITH MISTRAL ===
Evaluating llama: La resposta √©s precisa i ben estructurada...

=== COMBINING BEST RESPONSES ===
üíª Combined response generated and saved!
üìã Complete history in: ./results/complete_history.txt
```

## Estructura d'Arxius Generats

```
results/
‚îú‚îÄ‚îÄ responses/
‚îÇ   ‚îú‚îÄ‚îÄ mistral1.txt, mistral2.txt, mistral3.txt...
‚îÇ   ‚îú‚îÄ‚îÄ llama1.txt, llama2.txt, llama3.txt...
‚îÇ   ‚îú‚îÄ‚îÄ codellama1.txt, codellama2.txt...
‚îÇ   ‚îî‚îÄ‚îÄ response_combined_final.txt
‚îî‚îÄ‚îÄ complete_history.txt
```

## Funcionalitats Avan√ßades

### Detecci√≥ Autom√†tica del Tipus de Contingut

El sistema detecta autom√†ticament el tipus de contingut i optimitza segons el context:

- **Codi**: `python`, `javascript`, `bash`, `c++`
- **Llistes**: Instruccions pas a pas
- **Poesia**: Haikus, versos, estrofes
- **Di√†legs**: Converses, guions
- **Text general**: Explicacions, assaigs

### Sistema d'Avaluaci√≥

Cada model avalua les respostes utilitzant criteris espec√≠fics:
- **Precisi√≥ t√®cnica**
- **Claredat d'explicaci√≥**
- **Completesa de la resposta**
- **Rellev√†ncia al context**

### Recomanacions Contextuals

```bash
# Per a codi
üíª Recomanaci√≥: Executa 'python3 resposta_final.py' per provar

# Per a llistes
üìã Recomanaci√≥: Desa-ho com a PDF o compara com a instruccions

# Per a poesia
üé≠ Recomanaci√≥: Perfecte per a an√†lisi literari

# Per a di√†legs
üé¨ Recomanaci√≥: Ideal per a guions o role-playing
```

## Configuraci√≥ Avan√ßada

### Par√†metres de Model
```bash
# Edita local-cros.sh per ajustar par√†metres
-n 200           # Nombre m√†xim de tokens
--temp 0.7       # Temperatura (creativitat)
--top-k 40       # Top-k sampling
--top-p 0.9      # Top-p sampling
--repeat-penalty 1.1  # Penalitzaci√≥ per repetici√≥
```

### Personalitzaci√≥ de l'Avaluaci√≥
```bash
# Modificar el prompt d'avaluaci√≥ en la funci√≥ evaluate_response()
local evaluation_prompt="Avalua aquesta resposta utilitzant aquests criteris..."
```

## Casos d'√ös

### 1. Desenvolupament de Programari
```bash
./local-cros.sh "Optimitza aquest algoritme d'ordenament bombolla"
# Obtingues m√∫ltiples enfocaments d'optimitzaci√≥
```

### 2. Escriptura Creativa
```bash
./local-cros.sh "Escriu un di√†leg entre S√≤crates i Steve Jobs sobre √®tica"
# Combina diferents estils narratius
```

### 3. An√†lisi T√®cnica
```bash
./local-cros.sh "Explica els avantatges i desavantatges dels microserveis"
# M√∫ltiples perspectives t√®cniques combinades
```

### 4. Resoluci√≥ de Problemes
```bash
./local-cros.sh "Com debuggear un memory leak en C++"
# Diferents enfocaments de debugging
```

## M√®triques i An√†lisi

### Historial Complet
L'arxiu `complete_history.txt` cont√©:
```
#=== EXECUTION 2025-01-21 15:30:15 ===
MODEL: mistral1
QUESTION: ¬øQu√® √©s machine learning?
RESPONSE: Machine learning √©s una branca de la IA...

#=== EVALUATION 2025-01-21 15:30:45 ===
EVALUATOR: llama
EVALUATING: Machine learning √©s una branca...
RESULT: Resposta precisa i ben estructurada...

#=== COMBINED RESPONSE 2025-01-21 15:31:00 ===
TYPE: text_general
COMBINATION: Machine learning √©s una disciplina...
```

### An√†lisi de Tend√®ncies
```bash
# Comptar respostes per model
grep -c "MODEL:" results/complete_history.txt

# Veure evoluci√≥ temporal
grep "EXECUTION" results/complete_history.txt | tail -10
```

## Soluci√≥ de Problemes

### Error: "llama-cli not found"
```bash
# Verificar instal¬∑laci√≥
which llama-cli

# Actualitzar configuraci√≥
vim local-cros.conf
```

### Error: "Model execution failed"
```bash
# Verificar model
ls -la /path/to/your/model.gguf

# Provar manualment
/path/to/llama-cli -m /path/to/model.gguf -p "test"
```

### Respostes de Baixa Qualitat
```bash
# Ajustar par√†metres en el script
--temp 0.5        # Menys creativitat, m√©s precisi√≥
-n 500            # M√©s tokens per a respostes completes
```

## Extensions i Plugins

### Afegir Nou Model
1. Edita `local-cros.conf`
2. Afegir `MODEL_N_NAME` i `MODEL_N_PATH`
3. Reiniciar script

### Integraci√≥ amb APIs Externes
```bash
# Exemple: integrar amb Claude API per a avaluaci√≥ externa
curl -X POST "https://api.anthropic.com/v1/messages" \
  -H "Content-Type: application/json" \
  -d '{"model": "claude-3-sonnet", "messages": [...]}'
```

## Contribuci√≥

1. Fork del repositori
2. Crear branca: `git checkout -b feature/nova-funcionalitat`
3. Commit: `git commit -am 'Afegir funcionalitat X'`
4. Push: `git push origin feature/nova-funcionalitat`
5. Pull Request

## Llic√®ncia

MIT License

## Autor

**Gustavo Silva da Costa**

## Versi√≥

**1.0.0** - Sistema d'avaluaci√≥ creuada i s√≠ntesi intel¬∑ligent

---

*Local-CROS: On m√∫ltiples ments artificials col¬∑laboren per generar respostes superiors.*
