ğŸ¤– Local-CROS : SystÃ¨me dâ€™Optimisation par RÃ©fÃ©rences CroisÃ©es

Description

Local-CROS est un systÃ¨me avancÃ© dâ€™Ã©valuation croisÃ©e pour modÃ¨les LLaMA locaux, permettant de comparer les rÃ©ponses de plusieurs modÃ¨les et de gÃ©nÃ©rer des rÃ©ponses optimisÃ©es grÃ¢ce Ã  une synthÃ¨se intelligente.
Le systÃ¨me met en Å“uvre une approche unique dâ€™Ã©valuation mutuelle oÃ¹ chaque modÃ¨le Ã©value les rÃ©ponses des autres.

â¸»

âœ³ï¸ FonctionnalitÃ©s Principales

ğŸ”„ Ã‰valuation CroisÃ©e
	â€¢	Ã‰valuation mutuelle : chaque modÃ¨le Ã©value les rÃ©ponses de tous les autres
	â€¢	Multiples perspectives : obtenez diffÃ©rentes approches pour une mÃªme question
	â€¢	Score automatique : systÃ¨me de notation automatique pour chaque rÃ©ponse
	â€¢	Historique complet : enregistrement dÃ©taillÃ© de toutes les interactions

ğŸ¯ SynthÃ¨se Intelligente
	â€¢	DÃ©tection automatique du type de contenu : code, listes, poÃ©sie, dialogues, etc.
	â€¢	Combinaison optimisÃ©e : fusionne les meilleures parties de chaque rÃ©ponse
	â€¢	Ã‰limination des redondances : supprime les informations rÃ©pÃ©tÃ©es
	â€¢	Recommandations contextuelles : suggestions adaptÃ©es selon le contenu

ğŸ“Š SystÃ¨me de Fichiers IncrÃ©mental
	â€¢	NumÃ©rotation automatique : modele1.txt, modele2.txt, etc.
	â€¢	Historique cumulatif : toutes les exÃ©cutions enregistrÃ©es dans un fichier central
	â€¢	Horodatage dÃ©taillÃ© : trace temporelle de chaque opÃ©ration
	â€¢	TraÃ§abilitÃ© complÃ¨te : suivi intÃ©gral de lâ€™Ã©volution

â¸»

âš™ï¸ PrÃ©requis SystÃ¨me
	â€¢	llama.cpp compilÃ© et fonctionnel
	â€¢	2 Ã  4 modÃ¨les GGUF compatibles
	â€¢	Bash 4.0+
	â€¢	Outils requis : find, sed, sort, jq (optionnel)
	â€¢	SystÃ¨me dâ€™exploitation : macOS, Linux

â¸»

ğŸ§© Installation

1. TÃ©lÃ©chargement

# Cloner le dÃ©pÃ´t
git clone https://github.com/ton-utilisateur/local-cros.git
cd local-cros

# Rendre exÃ©cutable
chmod +x local-cros.sh

2. PremiÃ¨re Configuration

# ExÃ©cution initiale (configuration interactive)
./local-cros.sh

Le script vous demandera :
	â€¢	Chemin de llama-cli : emplacement du binaire llama.cpp
	â€¢	RÃ©pertoire de travail : dossier pour sauvegarder les rÃ©sultats
	â€¢	Configuration des modÃ¨les : nom et chemin de chaque modÃ¨le (2 Ã  4 modÃ¨les)

3. Fichier de Configuration GÃ©nÃ©rÃ©

# local-cros.conf
LLAMA_CLI_PATH="/chemin/vers/llama-cli"
WORK_DIR="./results"

MODEL_1_NAME="mistral"
MODEL_1_PATH="/chemin/vers/mistral.gguf"

MODEL_2_NAME="llama"
MODEL_2_PATH="/chemin/vers/llama.gguf"
# ... etc


â¸»

ğŸš€ Utilisation

Mode Interactif

./local-cros.sh
What do you need?
> Ã‰cris un poÃ¨me Ã©pique sur la programmation en Python

Mode Commande Directe

./local-cros.sh "Explique les diffÃ©rences entre React et Vue.js"

Exemple de Sortie

ğŸ¤– DÃ©marrage de la comparaison de modÃ¨les : "Explique la programmation fonctionnelle"

==> Consultation de mistral...
[mistral] dit : La programmation fonctionnelle est un paradigme...
---

==> Consultation de llama...
[llama] dit : En programmation fonctionnelle, les fonctions sont...
---

=== Ã‰VALUATION CROISÃ‰E ENTRE MODÃˆLES ===
=== Ã‰VALUATION AVEC MISTRAL ===
Ã‰valuation de llama : la rÃ©ponse est prÃ©cise et bien structurÃ©e...

=== COMBINAISON DES MEILLEURES RÃ‰PONSES ===
ğŸ’» RÃ©ponse combinÃ©e gÃ©nÃ©rÃ©e et enregistrÃ©e !
ğŸ“‹ Historique complet : ./results/complete_history.txt


â¸»

ğŸ“ Structure des Fichiers GÃ©nÃ©rÃ©s

results/
â”œâ”€â”€ responses/
â”‚   â”œâ”€â”€ mistral1.txt, mistral2.txt, mistral3.txt...
â”‚   â”œâ”€â”€ llama1.txt, llama2.txt, llama3.txt...
â”‚   â”œâ”€â”€ codellama1.txt, codellama2.txt...
â”‚   â””â”€â”€ response_combined_final.txt
â””â”€â”€ complete_history.txt


â¸»

ğŸ§  FonctionnalitÃ©s AvancÃ©es

DÃ©tection Automatique du Type de Contenu

Le systÃ¨me dÃ©tecte automatiquement le type de contenu et ajuste son optimisation :
	â€¢	Code : python, javascript, bash, c++
	â€¢	Listes : Ã©tapes, procÃ©dures
	â€¢	PoÃ©sie : haÃ¯kus, vers, strophes
	â€¢	Dialogues : conversations, scÃ©narios
	â€¢	Texte gÃ©nÃ©ral : explications, essais

SystÃ¨me dâ€™Ã‰valuation

Chaque modÃ¨le Ã©value les rÃ©ponses selon plusieurs critÃ¨res :
	â€¢	PrÃ©cision technique
	â€¢	ClartÃ© de lâ€™explication
	â€¢	ExhaustivitÃ©
	â€¢	Pertinence contextuelle

Recommandations Contextuelles

# Pour le code
ğŸ’» Recommandation : ExÃ©cuter 'python3 reponse_finale.py' pour tester

# Pour les listes
ğŸ“‹ Recommandation : Enregistrer en PDF ou partager comme guide

# Pour la poÃ©sie
ğŸ­ Recommandation : IdÃ©al pour analyse littÃ©raire

# Pour les dialogues
ğŸ¬ Recommandation : Parfait pour les scÃ©narios ou jeux de rÃ´le


â¸»

âš™ï¸ Configuration AvancÃ©e

ParamÃ¨tres du ModÃ¨le

# Modifier local-cros.sh pour ajuster les paramÃ¨tres
-n 200           # Nombre maximal de tokens
--temp 0.7       # TempÃ©rature (crÃ©ativitÃ©)
--top-k 40       # Ã‰chantillonnage top-k
--top-p 0.9      # Ã‰chantillonnage top-p
--repeat-penalty 1.1  # PÃ©nalisation de rÃ©pÃ©tition

Personnalisation de lâ€™Ã‰valuation

# Modifier le prompt dâ€™Ã©valuation dans la fonction evaluate_response()
local evaluation_prompt="Ã‰value cette rÃ©ponse selon ces critÃ¨res..."


â¸»

ğŸ’¡ Cas dâ€™Utilisation

1. DÃ©veloppement Logiciel

./local-cros.sh "Optimise cet algorithme de tri Ã  bulles"
# Obtiens plusieurs approches dâ€™optimisation

2. Ã‰criture CrÃ©ative

./local-cros.sh "Ã‰cris un dialogue entre Socrate et Steve Jobs sur lâ€™Ã©thique"
# Combine diffÃ©rents styles narratifs

3. Analyse Technique

./local-cros.sh "Explique les avantages et inconvÃ©nients des microservices"
# Fusionne plusieurs perspectives techniques

4. RÃ©solution de ProblÃ¨mes

./local-cros.sh "Comment dÃ©boguer une fuite mÃ©moire en C++"
# DiffÃ©rentes stratÃ©gies de dÃ©bogage


â¸»

ğŸ“ˆ MÃ©triques et Analyse

Historique Complet

Le fichier complete_history.txt contient :

#=== EXÃ‰CUTION 2025-01-21 15:30:15 ===
MODEL: mistral1
QUESTION: Quâ€™est-ce que le machine learning ?
RESPONSE: Le machine learning est une branche de lâ€™IA...

#=== Ã‰VALUATION 2025-01-21 15:30:45 ===
EVALUATOR: llama
EVALUATING: Le machine learning est une branche...
RESULT: RÃ©ponse prÃ©cise et bien structurÃ©e...

#=== RÃ‰PONSE COMBINÃ‰E 2025-01-21 15:31:00 ===
TYPE: texte_gÃ©nÃ©ral
COMBINATION: Le machine learning est une discipline...

Analyse des Tendances

# Compter les rÃ©ponses par modÃ¨le
grep -c "MODEL:" results/complete_history.txt

# Voir lâ€™Ã©volution temporelle
grep "EXÃ‰CUTION" results/complete_history.txt | tail -10


â¸»

ğŸ§© DÃ©pannage

Erreur : â€œllama-cli not foundâ€

which llama-cli
vim local-cros.conf

Erreur : â€œModel execution failedâ€

ls -la /chemin/vers/votre/model.gguf
/path/to/llama-cli -m /chemin/vers/model.gguf -p "test"

RÃ©ponses de Faible QualitÃ©

--temp 0.5        # Moins de crÃ©ativitÃ©, plus de prÃ©cision
-n 500            # Plus de tokens pour rÃ©ponses complÃ¨tes


â¸»

ğŸ”Œ Extensions et Plugins

Ajouter un Nouveau ModÃ¨le
	1.	Modifier local-cros.conf
	2.	Ajouter MODEL_N_NAME et MODEL_N_PATH
	3.	Relancer le script

IntÃ©gration avec des APIs Externes

# Exemple : intÃ©gration avec lâ€™API Claude pour Ã©valuation externe
curl -X POST "https://api.anthropic.com/v1/messages" \
  -H "Content-Type: application/json" \
  -d '{"model": "claude-3-sonnet", "messages": [...]}'


â¸»

ğŸ¤ Contribution
	1.	Fork du dÃ©pÃ´t
	2.	CrÃ©er une branche : git checkout -b feature/nouvelle-fonctionnalitÃ©
	3.	Commit : git commit -am 'Ajout de la fonctionnalitÃ© X'
	4.	Push : git push origin feature/nouvelle-fonctionnalitÃ©
	5.	Pull Request

â¸»

ğŸ“œ Licence

Licence MIT

â¸»

ğŸ‘¤ Auteur

Gustavo Silva da Costa

â¸»

ğŸ§¾ Version

1.0.0 â€“ SystÃ¨me dâ€™Ã©valuation croisÃ©e et synthÃ¨se intelligente

â¸»

Local-CROS : LÃ  oÃ¹ plusieurs esprits artificiels collaborent pour produire des rÃ©ponses supÃ©rieures.