ğŸ¤– MCP Local - Chat IA avec Outils SystÃ¨me

SystÃ¨me complet Model Context Protocol avec 11 outils et mode agentique pour votre IA locale

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                            â•‘
â•‘       Transformez votre LLM local en un assistant puissant â•‘
â•‘       avec accÃ¨s Ã  votre systÃ¨me d'exploitation            â•‘
â•‘                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â¸»

ğŸ“‹ Table des matiÃ¨res
	â€¢	Quâ€™est-ce que câ€™est ?
	â€¢	CaractÃ©ristiques
	â€¢	PrÃ©requis
	â€¢	Installation
	â€¢	Utilisation de base
	â€¢	Mode agentique
	â€¢	Les 11 outils
	â€¢	Exemples pratiques
	â€¢	Configuration avancÃ©e
	â€¢	DÃ©pannage
	â€¢	Architecture
	â€¢	CrÃ©dits

â¸»

ğŸ¯ Quâ€™est-ce que câ€™est ?

MCP Local est un systÃ¨me qui connecte votre modÃ¨le de langage local (comme Mistral, Llama, etc.) Ã  de vÃ©ritables outils de votre systÃ¨me dâ€™exploitation.

Sans MCP :

ğŸ‘¤ Utilisateur : "Liste mes fichiers Python"
ğŸ¤– IA : "DÃ©solÃ©, je ne peux pas accÃ©der Ã  votre systÃ¨me de fichiers"

Avec MCP :

ğŸ‘¤ Utilisateur : "Liste mes fichiers Python"
ğŸ¤– IA : [RECHERCHER] âœ“
       J'ai trouvÃ© 12 fichiers : main.py, utils.py, config.py...

Câ€™est comme donner des mains Ã  votre IA pour quâ€™elle interagisse avec votre ordinateur ğŸ¦¾

â¸»

âœ¨ CaractÃ©ristiques

ğŸ”§ 11 outils complets
	â€¢	âœ… Lire et Ã©crire des fichiers
	â€¢	âœ… ExÃ©cuter des commandes bash
	â€¢	âœ… Parcourir les rÃ©pertoires
	â€¢	âœ… Rechercher des fichiers et du contenu
	â€¢	âœ… Interroger des API HTTP
	â€¢	âœ… TÃ©lÃ©charger des fichiers depuis des URL
	â€¢	âœ… Compresser/dÃ©compresser (zip, tar, tar.gz)
	â€¢	âœ… OpÃ©rations Git (status, log, diff, branch)
	â€¢	âœ… Surveillance systÃ¨me (RAM, CPU, disque)
	â€¢	âœ… Recherche de contenu (grep)

ğŸ§  Mode agentique

La fonctionnalitÃ© phare ! Lâ€™IA peut enchaÃ®ner plusieurs actions automatiquement :

ğŸ‘¤ : "tÃ©lÃ©charge le README de GitHub et compresse tous les markdown"

ğŸ¤– [MODE AGENTIQUE]
   ğŸ“‹ Plan : 3 Ã©tapes
   ğŸ”„ TÃ©lÃ©chargement... âœ…
   ğŸ”„ Recherche *.md... âœ…  
   ğŸ”„ Compression... âœ…
   
   âœ… J'ai tÃ©lÃ©chargÃ© le README (3.4KB), trouvÃ© 5 markdown
      et les ai compressÃ©s en docs.zip (45KB)

ğŸ”’ SÃ©curitÃ© intÃ©grÃ©e
	â€¢	âŒ Commandes dangereuses bloquÃ©es (rm, dd, sudo, etc.)
	â€¢	ğŸ›¡ï¸ Ã‰criture limitÃ©e Ã  $HOME ou /tmp
	â€¢	â±ï¸ Timeouts automatiques
	â€¢	ğŸ“¦ Limites de taille de fichier (10MB)

ğŸ¨ Interface conviviale
	â€¢	ğŸ’¬ Chat interactif
	â€¢	ğŸ“Š Mode verbose pour dÃ©bogage
	â€¢	ğŸ¯ DÃ©tection automatique du mode agentique
	â€¢	âš¡ RÃ©ponses rapides et claires

â¸»

ğŸ“¦ PrÃ©requis

Avant dâ€™installer, assurez-vous dâ€™avoir :

Obligatoire

âœ… Python 3.8 ou supÃ©rieur
âœ… pip3
âœ… Un modÃ¨le GGUF (Mistral, Llama, etc.)
âœ… llama.cpp compilÃ© avec llama-cli

Optionnel

ğŸ”§ git (pour l'outil Git)
ğŸ”§ curl/wget (inclus sur macOS/Linux)

SystÃ¨mes supportÃ©s
	â€¢	âœ… macOS (testÃ©)
	â€¢	âœ… Linux (testÃ©)
	â€¢	âš ï¸ Windows (via WSL)

â¸»

ğŸš€ Installation

Ã‰tape 1 : TÃ©lÃ©charger lâ€™installateur

# Option A : Cloner le dÃ©pÃ´t
git clone https://github.com/tu-repo/mcp-local.git
cd mcp-local

# Option B : TÃ©lÃ©charger le script directement
curl -O https://tu-url/mcp_setup.sh
chmod +x mcp_setup.sh

Ã‰tape 2 : ExÃ©cuter lâ€™installateur

./mcp_setup.sh

Ã‰tape 3 : Configurer les chemins

Lâ€™installateur vous demandera deux chemins :

ğŸ¯ CONFIGURATION INITIALE
==========================================

ğŸ“ Ã‰tape 1/2 : Chemin de l'exÃ©cutable llama-cli
   Exemple : /usr/local/bin/llama-cli
   ou : /Users/ton-utilisateur/llama.cpp/build/bin/llama-cli
   Chemin complet : _

ğŸ“ Ã‰tape 2/2 : Chemin du modÃ¨le GGUF
   Exemple : /Users/ton-utilisateur/modeles/mistral-7b-instruct.gguf
   Chemin complet : _

Ã‰tape 4 : Installation automatique

Le script effectuera automatiquement :
	1.	âœ… CrÃ©ation dâ€™un environnement virtuel Python
	2.	âœ… Installation des dÃ©pendances (flask, psutil, requests)
	3.	âœ… GÃ©nÃ©ration dâ€™un serveur MCP (11 outils)
	4.	âœ… GÃ©nÃ©ration dâ€™un client avec mode agentique
	5.	âœ… Sauvegarde de la configuration

âœ… INSTALLATION TERMINÃ‰E

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     MCP LOCAL - MENU PRINCIPAL         â•‘
â•‘     ğŸ’ª 11 Outils + Mode agentique       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  1) ğŸ’¬ DÃ©marrer le chat (avec mode agentique)
  2) ğŸ”§ Voir les outils MCP (11)
  3) âš™ï¸  Reconfigurer les chemins
  4) ğŸšª Quitter


â¸»

ğŸ’¬ Utilisation de base

DÃ©marrer le chat

./mcp_setup.sh
# Choisir l'option 1) DÃ©marrer le chat

Commandes du chat

ğŸ‘¤ Vous : _

Commandes disponibles :
  agentico on/off  â†’ Activer/dÃ©sactiver le mode agentique
  verbose on/off   â†’ Afficher les logs dÃ©taillÃ©s
  herramientas     â†’ Lister les 11 outils
  salir            â†’ Fermer le chat

Exemple de conversation normale

ğŸ‘¤ Vous : liste les fichiers sur mon bureau

ğŸ¤– IA: [LISTER] âœ“
   Vous avez 23 Ã©lÃ©ments sur votre bureau : Documents/, Downloads/,
   image.png, notes.txt...

ğŸ‘¤ Vous : combien de RAM libre ai-je ?

ğŸ¤– IA: [MEMOIRE] âœ“
   Vous avez 8.5GB de RAM libre sur 16GB (53% libre)


â¸»

ğŸ§  Mode agentique

Le mode agentique permet Ã  lâ€™IA dâ€™enchaÃ®ner plusieurs actions automatiquement sans que vous donniez chaque commande sÃ©parÃ©ment.

Comment lâ€™activer ?

Option 1 : Manuelle

ğŸ‘¤ Vous : agentico on
ğŸ¤– Mode agentique : ACTIVÃ‰

Option 2 : Automatique (dÃ©tection par mots-clÃ©)
	â€¢	et puis
	â€¢	ensuite
	â€¢	et compresse
	â€¢	et recherche
	â€¢	faites tout
	â€¢	automatique

Exemple complet

Sans mode agentique (3 commandes sÃ©parÃ©es) :

ğŸ‘¤: tÃ©lÃ©charge le README
ğŸ¤–: âœ“

ğŸ‘¤: recherche tous les markdown
ğŸ¤–: âœ“

ğŸ‘¤: compresse les fichiers
ğŸ¤–: âœ“

Avec mode agentique (1 seule commande) :

ğŸ‘¤: tÃ©lÃ©charge le README de GitHub et ensuite compresse tous les markdown

ğŸ¤– [MODE AGENTIQUE ACTIVÃ‰]
ğŸ“‹ Plan : 3 Ã©tapes

ğŸ”„ Ã‰tape 1/3 : TÃ‰LÃ‰CHARGER:https://raw.githubusercontent.com/...
   âœ… TÃ‰LÃ‰CHARGÃ‰

ğŸ”„ Ã‰tape 2/3 : RECHERCHER:~/Desktop:*.md
   âœ… TROUVÃ‰

ğŸ”„ Ã‰tape 3/3 : COMPRESSER:~/Desktop â†’ ~/Desktop/docs.zip
   âœ… COMPRESSÃ‰

ğŸ”„ SynthÃ¨se des rÃ©sultats...

âœ… TÃ¢che complÃ©tÃ©e

ğŸ¤– J'ai tÃ©lÃ©chargÃ© le README (3456 octets), trouvÃ© 5 fichiers 
   markdown sur votre bureau et les ai compressÃ©s en docs.zip 
   (45KB). Tout est prÃªt !

Mode verbose (debug)

Pour visualiser le processus interne :

ğŸ‘¤ Vous : verbose on
ğŸ“Š Mode verbose : ACTIVÃ‰

ğŸ‘¤ Vous : tÃ©lÃ©charge X et compresse Y

ğŸ§  Planification des Ã©tapes...
ğŸ“‹ Ã‰tapes planifiÃ©es : ["TÃ‰LÃ‰CHARGER:...", "RECHERCHER:...", "COMPRESSER:..."]
ğŸ” ExÃ©cution : TÃ‰LÃ‰CHARGER:...
   âœ… TÃ‰LÃ‰CHARGÃ‰
ğŸ” ExÃ©cution : RECHERCHER:...
   âœ… TROUVÃ‰
...


â¸»

ğŸ› ï¸ Les 11 outils

1. ğŸ“– Lire un fichier

ğŸ‘¤: lis le fichier README.md
ğŸ¤–: [LIRE] âœ“
   Le fichier contient la documentation sur...

	â€¢	ğŸ“¦ Max : 64KB
	â€¢	ğŸ”’ Texte seulement

2. âœï¸ Ã‰crire un fichier

ğŸ‘¤: crÃ©e test.txt avec "Bonjour le monde"
ğŸ¤–: [Ã‰CRIRE] âœ“ (11 octets)
   Fichier crÃ©Ã© dans ~/test.txt

	â€¢	ğŸ“¦ Max : 10MB
	â€¢	ğŸ”’ Uniquement $HOME ou /tmp
	â€¢	ğŸ”€ Modes : w (Ã©craser) ou a (ajouter)

3. ğŸ“ Lister un rÃ©pertoire

ğŸ‘¤: que contient mon dossier Downloads ?
ğŸ¤–: [LISTER] âœ“
   45 Ã©lÃ©ments : documents/, images/, video.mp4...

	â€¢	ğŸ“Š Affiche : nom, type, taille, date
	â€¢	ğŸ“¦ Limite : 100 items

4. ğŸ” Rechercher des fichiers

ğŸ‘¤: trouve tous mes fichiers Python
ğŸ¤–: [RECHERCHER] âœ“
   12 fichiers trouvÃ©s : main.py, utils.py...

	â€¢	ğŸŒ² Recherche rÃ©cursive
	â€¢	ğŸ¯ Patterns glob : *.py, test*.txt, etc.
	â€¢	ğŸ“¦ Limite : 50 fichiers

5. ğŸ” Rechercher dans le contenu (Grep)

ğŸ‘¤: cherche "TODO" dans les fichiers Python
ğŸ¤–: [GREP] âœ“ (8 correspondances)
   main.py:42: # TODO: ImplÃ©menter validation
   utils.py:15: # TODO: Optimiser l'algorithme

	â€¢	ğŸ“„ Uniquement fichiers <1MB
	â€¢	ğŸ¯ Regex insensible Ã  la casse
	â€¢	ğŸ“¦ Limite : 50 correspondances

6. âš¡ ExÃ©cuter une commande

ğŸ‘¤: exÃ©cute ls -la
ğŸ¤–: [COMMANDE] âœ“
   total 256
   drwxr-xr-x  15 user  staff   480 Oct 10 10:30 .
   ...

	â€¢	âŒ BloquÃ© : rm, dd, sudo, su, mkfs
	â€¢	â±ï¸ Timeout : 10 secondes
	â€¢	ğŸ“¦ Sortie : max 4KB

7. ğŸ’¾ Consulter la mÃ©moire / ressources

ğŸ‘¤: quelles ressources ai-je de dispo ?
ğŸ¤–: [MEMOIRE] âœ“
   RAM : 8.5GB libre sur 16GB
   CPU : 35% d'utilisation (8 cÅ“urs)
   Disque : 245GB libres sur 500GB

8. ğŸ“¥ TÃ©lÃ©charger un fichier

ğŸ‘¤: tÃ©lÃ©charge https://example.com/file.pdf
ğŸ¤–: [TÃ‰LÃ‰CHARGER] âœ“ (2.5MB)
   Fichier enregistrÃ© dans ~/Downloads/file.pdf

	â€¢	ğŸŒ Protocoles : http:// et https://
	â€¢	ğŸ“¦ Limite : 10MB
	â€¢	â±ï¸ Timeout : 30 secondes

9. ğŸ—œï¸ Compresser / DÃ©compresser

ğŸ‘¤: compresse mon dossier documents
ğŸ¤–: [COMPRESSER] âœ“
   Dossier compressÃ© en documents.zip (12MB)

	â€¢	ğŸ“¦ Formats : zip, tar, tar.gz
	â€¢	ğŸ”„ OpÃ©rations : compresser, dÃ©compresser

10. ğŸ”§ OpÃ©rations Git

ğŸ‘¤: montre le status git
ğŸ¤–: [GIT] âœ“
   On branch main
   Changes not staged for commit:
     modified: src/main.py

	â€¢	âœ… AutorisÃ© : status, log, diff, branch, remote
	â€¢	âŒ BloquÃ© : commit, push, pull (lecture seule)

11. ğŸŒ Interroger une API

ğŸ‘¤: interroge l'API example.com
ğŸ¤–: [API] âœ“ (200)
   Status: OK
   RÃ©ponse: {"data": "exemple"}

	â€¢	ğŸ“¡ MÃ©thodes : GET, POST, PUT, DELETE, PATCH
	â€¢	â±ï¸ Timeout : 10 secondes
	â€¢	ğŸ“¦ RÃ©ponse : max 4KB

â¸»

ğŸ’¡ Exemples pratiques

Exemple 1 : Analyse de projet

ğŸ‘¤: agentico on

ğŸ‘¤: liste tous les fichiers Python et cherche les TODOs

ğŸ¤– [MODE AGENTIQUE]
   Ã‰tape 1 : Recherche *.py... âœ…
   Ã‰tape 2 : Recherche 'TODO'... âœ…
   
   J'ai trouvÃ© 12 fichiers Python avec 8 TODOs :
   - main.py : 3 TODOs
   - utils.py : 2 TODOs
   - config.py : 3 TODOs

Exemple 2 : Sauvegarde automatisÃ©e

ğŸ‘¤: trouve tous les fichiers importants et compresse en backup

ğŸ¤– [MODE AGENTIQUE]
   Ã‰tape 1 : Recherche des fichiers... âœ…
   Ã‰tape 2 : Compression... âœ…
   
   J'ai crÃ©Ã© backup.zip avec 45 fichiers (234MB)

Exemple 3 : Surveillance systÃ¨me

ğŸ‘¤: affiche le git status et les ressources systÃ¨me

ğŸ¤– [MODE AGENTIQUE]
   Ã‰tape 1 : Git status... âœ…
   Ã‰tape 2 : Consultation ressources... âœ…
   
   Git : 3 fichiers modifiÃ©s sur la branche main
   SystÃ¨me : RAM 45% libre, CPU 25%, Disque 50% libre

Exemple 4 : Workflow complet

ğŸ‘¤: tÃ©lÃ©charge le README GitHub, cherche sur mon bureau 
    et compresse tous les markdown trouvÃ©s

ğŸ¤– [MODE AGENTIQUE]
   ğŸ“‹ Plan : 3 Ã©tapes
   
   Ã‰tape 1 : TÃ©lÃ©chargement depuis GitHub... âœ… (3.4KB)
   Ã‰tape 2 : Recherche *.md sur Desktop... âœ… (5 fichiers)
   Ã‰tape 3 : Compression... âœ… (45KB)
   
   âœ… J'ai tÃ©lÃ©chargÃ© le README, trouvÃ© 5 markdown et 
      les ai compressÃ©s en docs.zip. Tout est sur votre bureau.


â¸»

âš™ï¸ Configuration avancÃ©e

Changer le modÃ¨le ou le chemin de llama-cli

./mcp_setup.sh
# Choisir l'option 3) Reconfigurer les chemins

Ã‰diter la configuration manuellement

nano ~/.mcp_local/config.env

# Configuration MCP Local
LLAMA_CLI="/chemin/vers/ton/llama-cli"
MODELE_GGUF="/chemin/vers/ton/modele.gguf"

Variables dâ€™environnement

# Activer le debug du serveur MCP
export MCP_DEBUG=1

# Lancer
./mcp_setup.sh

Structure des fichiers

~/.mcp_local/
â”œâ”€â”€ config.env           # Ta configuration
â”œâ”€â”€ venv/                # Environnement Python
â”œâ”€â”€ mcp_server.py        # Serveur avec 11 outils
â””â”€â”€ chat_mcp.py          # Client avec mode agentique


â¸»

ğŸ”§ DÃ©pannage

ProblÃ¨me : â€œllama-cli introuvableâ€

Solution :

# VÃ©rifier que llama.cpp est compilÃ©
cd ~/llama.cpp
cmake -B build
cmake --build build

# VÃ©rifier le chemin
ls ~/llama.cpp/build/bin/llama-cli

# Reconfigurer MCP
./mcp_setup.sh
# Option 3) Reconfigurer les chemins

ProblÃ¨me : â€œModÃ¨le introuvableâ€

Solution :

# VÃ©rifier que le modÃ¨le existe
ls ~/chemin/vers/ton/modele.gguf

# Si vous n'avez pas de modÃ¨le, tÃ©lÃ©chargez-en un
# Exemple : Mistral 7B
wget https://huggingface.co/...modele.gguf

# Reconfigurer
./mcp_setup.sh
# Option 3) Reconfigurer les chemins

ProblÃ¨me : â€œErreur lors de lâ€™installation des dÃ©pendances Pythonâ€

Solution :

# VÃ©rifier Python
python3 --version  # Doit Ãªtre >= 3.8

# Supprimer l'environnement virtuel corrompu
rm -rf ~/.mcp_local/venv

# RÃ©installer
./mcp_setup.sh

ProblÃ¨me : â€œLe mode agentique ne fonctionne pas correctementâ€

Solution :

# Activer le mode verbose pour diagnostiquer
ğŸ‘¤: verbose on
ğŸ‘¤: ta commande problÃ©matique

# Le mode agentique dÃ©pend de la qualitÃ© du modÃ¨le
# ModÃ¨les recommandÃ©s :
# - Mistral 7B Instruct (minimum)
# - Llama 3 8B Instruct (mieux)
# - Mixtral 8x7B (optimal)

ProblÃ¨me : â€œTimeout sur les requÃªtesâ€

Solution :

# Si le modÃ¨le est lent, augmenter le timeout
# Ã‰diter ~/.mcp_local/chat_mcp.py

# Ligne ~40 :
IA_CMD = [
    config.get('LLAMA_CLI', 'llama-cli'),
    "--model", config.get('MODELE_GGUF', ''),
    "--n-predict", "512",
    "--temp", "0.7",
    "--ctx-size", "4096"
]

# Ajouter GPU si disponible :
# "--n-gpu-layers", "35"

ProblÃ¨me : â€œCommande bloquÃ©e pour sÃ©curitÃ©â€

Solution :
Ceci est volontaire. Les commandes dangereuses sont bloquÃ©es :
	â€¢	âŒ rm -rf
	â€¢	âŒ dd
	â€¢	âŒ sudo
	â€¢	âŒ su

Si vous avez vraiment besoin dâ€™exÃ©cuter des commandes privilÃ©giÃ©es, faites-le manuellement en dehors du MCP.

â¸»

ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ğŸ‘¤ Utilisateur (VOUS)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ğŸ’¬ Client Chat (chat_mcp.py)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  ğŸ§  Mode agentique                  â”‚     â”‚
â”‚  â”‚  - Planification des Ã©tapes        â”‚     â”‚
â”‚  â”‚  - ExÃ©cution sÃ©quentielle          â”‚     â”‚
â”‚  â”‚  - SynthÃ¨se des rÃ©sultats          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ¤– ModÃ¨le LLM Local                  â”‚
â”‚     (Mistral, Llama, Mixtral, etc.)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ”§ Serveur MCP (mcp_server.py)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  11 Outils :                        â”‚     â”‚
â”‚  â”‚  âœ“ Fichiers (lire/Ã©crire)          â”‚     â”‚
â”‚  â”‚  âœ“ SystÃ¨me (mÃ©moire/commandes)     â”‚     â”‚
â”‚  â”‚  âœ“ RÃ©seau (API/tÃ©lÃ©chargements)    â”‚     â”‚
â”‚  â”‚  âœ“ Recherche (fichiers/contenu)    â”‚     â”‚
â”‚  â”‚  âœ“ Utilitaires (git/compression)   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ’» Votre SystÃ¨me d'Exploitation         â”‚
â”‚  (Fichiers, Commandes, Ressources)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flux dâ€™une requÃªte normale

1. Utilisateur envoie une commande
   ğŸ‘¤ "liste les fichiers Python"
   
2. Le client interroge le LLM
   ğŸ’¬ â†’ ğŸ¤– "Quelle outil utiliser ?"
   
3. Le LLM choisit l'outil
   ğŸ¤– â†’ ğŸ’¬ "[UTILISER_OUTIL:RECHERCHER:.:*.py]"
   
4. Le client appelle le serveur MCP
   ğŸ’¬ â†’ ğŸ”§ {"method": "rechercher_fichiers", ...}
   
5. Le serveur exÃ©cute l'outil
   ğŸ”§ â†’ ğŸ’» Recherche rÃ©elle sur le systÃ¨me
   
6. Le serveur renvoie les rÃ©sultats
   ğŸ”§ â†’ ğŸ’¬ {"result": ["main.py", ...]}
   
7. Le client renvoie les rÃ©sultats au LLM
   ğŸ’¬ â†’ ğŸ¤– "Fichiers trouvÃ©s : ..."
   
8. Le LLM gÃ©nÃ¨re une rÃ©ponse naturelle
   ğŸ¤– â†’ ğŸ’¬ "J'ai trouvÃ© 12 fichiers Python : ..."
   
9. L'utilisateur voit la rÃ©ponse
   ğŸ’¬ â†’ ğŸ‘¤ "J'ai trouvÃ© 12 fichiers Python : ..."

Flux du mode agentique

1. L'utilisateur donne une commande complexe
   ğŸ‘¤ "tÃ©lÃ©charge X puis compresse Y"
   
2. Le client dÃ©tecte le mode agentique
   ğŸ’¬ [DÃ©tecte mots-clÃ© "puis", "ensuite"]
   
3. Le LLM planifie les Ã©tapes
   ğŸ’¬ â†’ ğŸ¤– "DÃ©composer en Ã©tapes"
   ğŸ¤– â†’ ğŸ’¬ ["TÃ‰LÃ‰CHARGER:...", "RECHERCHER:...", "COMPRESSER:..."]
   
4. Le client exÃ©cute les Ã©tapes sÃ©quentiellement
   ğŸ’¬ â†’ ğŸ”§ Ã‰tape 1 : TÃ‰LÃ‰CHARGER âœ…
   ğŸ’¬ â†’ ğŸ”§ Ã‰tape 2 : RECHERCHER âœ…
   ğŸ’¬ â†’ ğŸ”§ Ã‰tape 3 : COMPRESSER âœ…
   
5. Le LLM synthÃ©tise les rÃ©sultats
   ğŸ’¬ â†’ ğŸ¤– "RÃ©sume tout ce qui a Ã©tÃ© fait"
   ğŸ¤– â†’ ğŸ’¬ "J'ai tÃ©lÃ©chargÃ©, recherchÃ© et compressÃ©..."
   
6. L'utilisateur reÃ§oit le rÃ©sumÃ© final
   ğŸ’¬ â†’ ğŸ‘¤ "âœ… TÃ¢che complÃ©tÃ©e : ..."


â¸»

ğŸ“š Ressources additionnelles

Model Context Protocol (MCP)
	â€¢	ğŸ“– SpÃ©cification MCP
	â€¢	ğŸ”— GitHub Anthropic MCP

ModÃ¨les recommandÃ©s
	â€¢	ğŸ¦™ Llama 3 8B Instruct
	â€¢	ğŸŒŸ Mistral 7B Instruct
	â€¢	ğŸš€ Mixtral 8x7B

llama.cpp
	â€¢	ğŸ”— GitHub llama.cpp
	â€¢	ğŸ“– Documentation

â¸»

ğŸ“ Cas dâ€™utilisation

Pour les dÃ©veloppeurs
	â€¢	âœ… Automatiser les tÃ¢ches rÃ©pÃ©titives
	â€¢	âœ… Analyser le code et trouver les TODOs
	â€¢	âœ… GÃ©rer des dÃ©pÃ´ts Git
	â€¢	âœ… GÃ©nÃ©rer de la documentation
	â€¢	âœ… Surveiller les ressources systÃ¨me

Pour les administrateurs systÃ¨me
	â€¢	âœ… Automatiser les backups
	â€¢	âœ… Surveiller les logs
	â€¢	âœ… GÃ©rer les fichiers de configuration
	â€¢	âœ… Rechercher dans les logs
	â€¢	âœ… Compresser/dÃ©compresser les fichiers

Pour les utilisateurs avancÃ©s
	â€¢	âœ… Organiser automatiquement les fichiers
	â€¢	âœ… TÃ©lÃ©charger et traiter du contenu web
	â€¢	âœ… Rechercher de lâ€™information dans des documents
	â€¢	âœ… Automatiser des workflows complexes
	â€¢	âœ… IntÃ©grer des APIs externes

â¸»

ğŸ¤ Contribuer

Vous avez des idÃ©es pour amÃ©liorer MCP Local ? Contribuez !

IdÃ©es dâ€™outils supplÃ©mentaires
	â€¢	ğŸ“§ Client mail
	â€¢	ğŸ“… IntÃ©gration calendrier
	â€¢	ğŸ—„ï¸ OpÃ©rations base de donnÃ©es
	â€¢	ğŸ³ IntÃ©gration Docker
	â€¢	ğŸ“Š GÃ©nÃ©ration de rapports

Comment contribuer
	1.	Forkez le projet
	2.	CrÃ©ez une branche (git checkout -b feature/nouvel-outil)
	3.	Committez vos modifications (git commit -am 'Ajout de l'outil X')
	4.	Pushez la branche (git push origin feature/nouvel-outil)
	5.	Ouvrez une Pull Request

â¸»

ğŸ“„ Licence

Ce projet est sous licence MIT. Utilisez-le librement, modifiez-le et partagez-le.

MIT License

Copyright (c) 2025 Gustavo Silva da Costa

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


â¸»

ğŸ™ Remerciements
	â€¢	Anthropic pour le concept de Model Context Protocol
	â€¢	La communautÃ© llama.cpp pour rendre lâ€™exÃ©cution locale de LLM possible
	â€¢	Tous ceux qui contribuent Ã  lâ€™Ã©cosystÃ¨me IA open source

â¸»

ğŸ“ Support

Des problÃ¨mes ? Des questions ? Des suggestions ?
	â€¢	ğŸ“§ Email : gsilvadacosta0@gmail.com
	â€¢	ğŸ†‡ Anciennement Twitter ğŸ˜‚ : https://x.com/bibliogalactic

â¸»


<div align="center">


â­ Si ce projet vous plaÃ®t, mettez-lui une Ã©toile sur GitHub â­

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                        â•‘
â•‘   Fait avec â¤ï¸ pour la communautÃ© IA locale           â•‘
â•‘                                                        â•‘
â•‘   "Donner des mains aux IAs, un outil Ã  la fois"      â•‘
â•‘                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ‘¨â€ğŸ’» CrÃ©Ã© par

Gustavo Silva Da Costa (Eto Demerzel) ğŸ¤«

ğŸš€ Transformer les IAs locales en assistants puissants

</div>



â¸»

Version : 1.0.0
DerniÃ¨re mise Ã  jour : Octobre 2025
Ã‰tat : âœ… Production

â¸»

