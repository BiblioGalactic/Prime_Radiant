# ğŸ¤– MCP Local - AI Chat with System Tools

> **Complete Model Context Protocol system with 11 tools and agentic mode for your local AI**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                            â•‘
â•‘       Transform your local LLM into a powerful assistant   â•‘
â•‘       with access to your operating system                 â•‘
â•‘                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“‹ Table of Contents

- [What is this?](#-what-is-this)
- [Features](#-features)
- [Requirements](#-requirements)
- [Installation](#-installation)
- [Basic Usage](#-basic-usage)
- [Agentic Mode](#-agentic-mode)
- [The 11 Tools](#-the-11-tools)
- [Practical Examples](#-practical-examples)
- [Advanced Configuration](#-advanced-configuration)
- [Troubleshooting](#-troubleshooting)
- [Architecture](#-architecture)
- [Credits](#-credits)

---

## ğŸ¯ What is this?

**MCP Local** is a system that connects your local language model (like Mistral, Llama, etc.) with **real tools from your operating system**.

### Without MCP:
```
ğŸ‘¤ User: "List my Python files"
ğŸ¤– AI: "Sorry, I cannot access your file system"
```

### With MCP:
```
ğŸ‘¤ User: "List my Python files"
ğŸ¤– AI: [SEARCH] âœ“
       Found 12 files: main.py, utils.py, config.py...
```

**It's like giving hands to your AI so it can interact with your computer** ğŸ¦¾

---

## âœ¨ Features

### ğŸ”§ 11 Complete Tools
- âœ… Read and write files
- âœ… Execute bash commands
- âœ… Navigate directories
- âœ… Search files and content
- âœ… Query HTTP APIs
- âœ… Download files from URLs
- âœ… Compress/decompress (zip, tar, tar.gz)
- âœ… Git operations (status, log, diff, branch)
- âœ… System monitoring (RAM, CPU, disk)
- âœ… Content search (grep)

### ğŸ§  Agentic Mode
**The star feature!** The AI can chain multiple actions automatically:

```
ğŸ‘¤: "download the README from GitHub and compress all markdown files"

ğŸ¤– [AGENTIC MODE]
   ğŸ“‹ Plan: 3 steps
   ğŸ”„ Downloading... âœ…
   ğŸ”„ Searching *.md... âœ…  
   ğŸ”„ Compressing... âœ…
   
   âœ… Downloaded README (3.4KB), found 5 markdown files
      and compressed them in docs.zip (45KB)
```

### ğŸ”’ Built-in Security
- âŒ Dangerous commands blocked (rm, dd, sudo, etc.)
- ğŸ›¡ï¸ Can only write in $HOME or /tmp
- â±ï¸ Automatic timeouts
- ğŸ“¦ File size limits (10MB)

### ğŸ¨ User-Friendly Interface
- ğŸ’¬ Interactive chat
- ğŸ“Š Verbose mode for debugging
- ğŸ¯ Automatic agentic mode detection
- âš¡ Quick and clear responses

---

## ğŸ“¦ Requirements

Before installing, make sure you have:

### Mandatory Requirements
```bash
âœ… Python 3.8 or higher
âœ… pip3
âœ… A GGUF model (Mistral, Llama, etc.)
âœ… llama.cpp compiled with llama-cli
```

### Optional Requirements
```bash
ğŸ”§ git (for Git tool)
ğŸ”§ curl/wget (already included in macOS/Linux)
```

### Operating System
- âœ… macOS (tested)
- âœ… Linux (tested)
- âš ï¸ Windows (with WSL)

---

## ğŸš€ Installation

### Step 1: Download the installer

```bash
# Option A: Clone the repository
git clone https://github.com/your-repo/mcp-local.git
cd mcp-local

# Option B: Download the script directly
curl -O https://your-url/mcp_setup.sh
chmod +x mcp_setup.sh
```

### Step 2: Run the installer

```bash
./mcp_setup.sh
```

### Step 3: Configure paths

The installer will ask for two paths:

```
ğŸ¯ INITIAL CONFIGURATION
==========================================

ğŸ“ Step 1/2: Path to llama-cli executable
   Example: /usr/local/bin/llama-cli
   or: /Users/your-user/llama.cpp/build/bin/llama-cli
   Full path: _

ğŸ“ Step 2/2: Path to GGUF model
   Example: /Users/your-user/models/mistral-7b-instruct.gguf
   Full path: _
```

### Step 4: Automatic installation

The script will automatically:
1. âœ… Create Python virtual environment
2. âœ… Install dependencies (flask, psutil, requests)
3. âœ… Generate MCP server (11 tools)
4. âœ… Generate client with agentic mode
5. âœ… Save configuration

```
âœ… INSTALLATION COMPLETED

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     MCP LOCAL - MAIN MENU              â•‘
â•‘     ğŸ’ª 11 Tools + Agentic              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  1) ğŸ’¬ Start chat (with agentic mode)
  2) ğŸ”§ View MCP tools (11)
  3) âš™ï¸  Reconfigure paths
  4) ğŸšª Exit
```

---

## ğŸ’¬ Basic Usage

### Start the Chat

```bash
./mcp_setup.sh
# Select option 1) Start chat
```

### Chat Commands

```
ğŸ‘¤ You: _

Available commands:
  agentic on/off  â†’ Enable/disable agentic mode
  verbose on/off  â†’ View detailed logs
  herramientas    â†’ List the 11 tools
  salir           â†’ Close chat
```

### Normal Conversation Example

```bash
ğŸ‘¤ You: list the files on my desktop

ğŸ¤– AI: [LIST] âœ“
   You have 23 items on your desktop: Documents/, Downloads/,
   image.png, notes.txt...

ğŸ‘¤ You: how much free RAM do I have?

ğŸ¤– AI: [MEMORY] âœ“
   You have 8.5GB of available RAM out of 16GB total (53% free)
```

---

## ğŸ§  Agentic Mode

Agentic mode allows the AI to **chain multiple actions automatically** without you having to give commands one by one.

### How to Activate?

**Option 1: Manual**
```bash
ğŸ‘¤ You: agentic on
ğŸ¤– Agentic mode: ACTIVATED
```

**Option 2: Automatic** (detects these keywords)
- `and then`
- `after that`
- `and compress`
- `and search`
- `complete everything`
- `do everything`
- `automatic`

### Complete Example

#### Without Agentic Mode (3 separate commands):
```bash
ğŸ‘¤: download the README
ğŸ¤–: âœ“

ğŸ‘¤: search all markdown files
ğŸ¤–: âœ“

ğŸ‘¤: compress the files
ğŸ¤–: âœ“
```

#### With Agentic Mode (1 single command):
```bash
ğŸ‘¤: download the README from GitHub and then compress all markdown files

ğŸ¤– [AGENTIC MODE ACTIVATED]
ğŸ“‹ Plan: 3 steps

ğŸ”„ Step 1/3: DOWNLOAD:https://raw.githubusercontent.com/...
   âœ… DOWNLOAD

ğŸ”„ Step 2/3: SEARCH:~/Desktop:*.md
   âœ… SEARCH

ğŸ”„ Step 3/3: COMPRESS:compress:~/Desktop:~/Desktop/docs.zip
   âœ… COMPRESS

ğŸ”„ Synthesizing results...

âœ… Task completed

ğŸ¤– Downloaded README (3456 bytes), found 5 markdown files 
   on your desktop and compressed them in docs.zip 
   (45KB total). All done!
```

### Verbose Mode (Debug)

To see the internal process:

```bash
ğŸ‘¤ You: verbose on
ğŸ“Š Verbose mode: ACTIVATED

ğŸ‘¤ You: download X and compress Y

ğŸ§  Planning steps...
ğŸ“‹ Planned steps: ["DOWNLOAD:...", "SEARCH:...", "COMPRESS:..."]
ğŸ” Executing: DOWNLOAD:https://...
   âœ… DOWNLOAD
ğŸ” Executing: SEARCH:~/Desktop:*.md
   âœ… SEARCH
...
```

---

## ğŸ› ï¸ The 11 Tools

### 1. ğŸ“– Read File
```bash
ğŸ‘¤: read the README.md file
ğŸ¤–: [READ] âœ“
   The file contains documentation about...
```
- ğŸ“¦ Maximum: 64KB
- ğŸ”’ Text files only

### 2. âœï¸ Write File
```bash
ğŸ‘¤: create a test.txt file with "Hello World"
ğŸ¤–: [WRITE] âœ“ (11 bytes)
   File created at ~/test.txt
```
- ğŸ“¦ Maximum: 10MB
- ğŸ”’ Only in $HOME or /tmp
- ğŸ”€ Modes: `w` (overwrite) or `a` (append)

### 3. ğŸ“ List Directory
```bash
ğŸ‘¤: what's in my Downloads folder?
ğŸ¤–: [LIST] âœ“
   45 items: documents/, images/, video.mp4...
```
- ğŸ“Š Shows: name, type, size, date
- ğŸ“¦ Limit: 100 items

### 4. ğŸ” Search Files
```bash
ğŸ‘¤: find all my Python files
ğŸ¤–: [SEARCH] âœ“
   12 files found: main.py, utils.py...
```
- ğŸŒ² Recursive search
- ğŸ¯ Glob patterns: `*.py`, `test*.txt`, etc.
- ğŸ“¦ Limit: 50 files

### 5. ğŸ” Search in Content (Grep)
```bash
ğŸ‘¤: search for "TODO" in Python files
ğŸ¤–: [GREP] âœ“ (8 matches)
   main.py:42: # TODO: Implement validation
   utils.py:15: # TODO: Optimize algorithm
```
- ğŸ“„ Files <1MB only
- ğŸ¯ Case-insensitive regex
- ğŸ“¦ Limit: 50 matches

### 6. âš¡ Execute Command
```bash
ğŸ‘¤: execute ls -la
ğŸ¤–: [COMMAND] âœ“
   total 256
   drwxr-xr-x  15 user  staff   480 Oct 10 10:30 .
   ...
```
- âŒ **Blocked**: rm, dd, sudo, su, mkfs
- â±ï¸ Timeout: 10 seconds
- ğŸ“¦ Output: 4KB maximum

### 7. ğŸ’¾ Query Memory
```bash
ğŸ‘¤: how many resources do I have available?
ğŸ¤–: [MEMORY] âœ“
   RAM: 8.5GB free out of 16GB
   CPU: 35% usage (8 cores)
   Disk: 245GB free out of 500GB
```

### 8. ğŸ“¥ Download File
```bash
ğŸ‘¤: download https://example.com/file.pdf
ğŸ¤–: [DOWNLOAD] âœ“ (2.5MB)
   File saved at ~/Downloads/file.pdf
```
- ğŸŒ http:// and https:// only
- ğŸ“¦ Limit: 10MB
- â±ï¸ Timeout: 30 seconds

### 9. ğŸ—œï¸ Compress/Decompress
```bash
ğŸ‘¤: compress my documents folder
ğŸ¤–: [COMPRESS] âœ“
   Folder compressed in documents.zip (12MB)
```
- ğŸ“¦ Formats: zip, tar, tar.gz
- ğŸ”„ Operations: compress, decompress

### 10. ğŸ”§ Git Operations
```bash
ğŸ‘¤: show git status
ğŸ¤–: [GIT] âœ“
   On branch main
   Changes not staged for commit:
     modified: src/main.py
```
- âœ… Allowed: status, log, diff, branch, remote
- âŒ **Blocked**: commit, push, pull (read-only)

### 11. ğŸŒ Query API
```bash
ğŸ‘¤: query the example.com API
ğŸ¤–: [API] âœ“ (200)
   Status: OK
   Response: {"data": "example"}
```
- ğŸ“¡ Methods: GET, POST, PUT, DELETE, PATCH
- â±ï¸ Timeout: 10 seconds
- ğŸ“¦ Response: 4KB maximum

---

## ğŸ’¡ Practical Examples

### Example 1: Project Analysis
```bash
ğŸ‘¤: agentic on

ğŸ‘¤: list all Python files and search for pending TODOs

ğŸ¤– [AGENTIC MODE]
   Step 1: Searching *.py... âœ…
   Step 2: Searching 'TODO'... âœ…
   
   Found 12 Python files with 8 pending TODOs:
   - main.py: 3 TODOs
   - utils.py: 2 TODOs
   - config.py: 3 TODOs
```

### Example 2: Automated Backup
```bash
ğŸ‘¤: search all important files and compress them in a backup

ğŸ¤– [AGENTIC MODE]
   Step 1: Searching files... âœ…
   Step 2: Compressing... âœ…
   
   Created backup.zip with 45 files (234MB total)
```

### Example 3: System Monitoring
```bash
ğŸ‘¤: show git status and system resources

ğŸ¤– [AGENTIC MODE]
   Step 1: Git status... âœ…
   Step 2: Querying resources... âœ…
   
   Git: 3 modified files on main branch
   System: RAM 45% free, CPU 25%, Disk 50% free
```

### Example 4: Complete Workflow
```bash
ğŸ‘¤: download the README from GitHub, search it on my desktop 
    and compress all markdown files you find

ğŸ¤– [AGENTIC MODE]
   ğŸ“‹ Plan: 3 steps
   
   Step 1: Downloading from GitHub... âœ… (3.4KB)
   Step 2: Searching *.md on Desktop... âœ… (5 files)
   Step 3: Compressing files... âœ… (45KB)
   
   âœ… Downloaded README, found 5 markdown files and 
      compressed them in docs.zip. Everything is on your desktop.
```

---

## âš™ï¸ Advanced Configuration

### Change Model or llama-cli Path

```bash
./mcp_setup.sh
# Select option 3) Reconfigure paths
```

### Manual Configuration Edit

```bash
nano ~/.mcp_local/config.env
```

```bash
# MCP Local Configuration
LLAMA_CLI="/path/to/your/llama-cli"
MODELO_GGUF="/path/to/your/model.gguf"
```

### Environment Variables

```bash
# Enable MCP server debug
export MCP_DEBUG=1

# Run
./mcp_setup.sh
```

### File Structure

```
~/.mcp_local/
â”œâ”€â”€ config.env           # Your configuration
â”œâ”€â”€ venv/                # Python environment
â”œâ”€â”€ mcp_server.py        # Server with 11 tools
â””â”€â”€ chat_mcp.py          # Client with agentic mode
```

---

## ğŸ”§ Troubleshooting

### Problem: "llama-cli not found"

**Solution:**
```bash
# Verify llama.cpp is compiled
cd ~/llama.cpp
cmake -B build
cmake --build build

# Verify path
ls ~/llama.cpp/build/bin/llama-cli

# Reconfigure MCP
./mcp_setup.sh
# Option 3) Reconfigure paths
```

### Problem: "Model not found"

**Solution:**
```bash
# Verify the model exists
ls ~/path/to/your/model.gguf

# If you don't have a model, download one
# Example: Mistral 7B
wget https://huggingface.co/...model.gguf

# Reconfigure
./mcp_setup.sh
# Option 3) Reconfigure paths
```

### Problem: "Error installing Python dependencies"

**Solution:**
```bash
# Verify Python
python3 --version  # Must be 3.8+

# Clean virtual environment
rm -rf ~/.mcp_local/venv

# Reinstall
./mcp_setup.sh
```

### Problem: "Agentic mode doesn't work well"

**Solution:**
```bash
# Use verbose mode to see what happens
ğŸ‘¤: verbose on
ğŸ‘¤: your problematic command

# Agentic mode depends on model quality
# Recommended models:
# - Mistral 7B Instruct (minimum)
# - Llama 3 8B Instruct (better)
# - Mixtral 8x7B (optimal)
```

### Problem: "Query timeout"

**Solution:**
```bash
# If the model is very slow, increase timeout
# Edit ~/.mcp_local/chat_mcp.py

# Line ~40:
IA_CMD = [
    config.get('LLAMA_CLI', 'llama-cli'),
    "--model", config.get('MODELO_GGUF', ''),
    "--n-predict", "512",
    "--temp", "0.7",
    "--ctx-size", "4096"
]

# Add GPU if available:
# "--n-gpu-layers", "35"
```

### Problem: "Command blocked for security"

**Solution:**
This is intentional. Dangerous commands are blocked:
- âŒ `rm -rf`
- âŒ `dd`
- âŒ `sudo`
- âŒ `su`

If you really need to execute privileged commands, do it manually outside of MCP.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ğŸ‘¤ User (YOU)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ğŸ’¬ Chat Client (chat_mcp.py)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  ğŸ§  Agentic Mode                   â”‚     â”‚
â”‚  â”‚  - Step planning                   â”‚     â”‚
â”‚  â”‚  - Sequential execution            â”‚     â”‚
â”‚  â”‚  - Result synthesis                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ¤– Local LLM Model                   â”‚
â”‚     (Mistral, Llama, Mixtral, etc.)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ”§ MCP Server (mcp_server.py)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  11 Tools:                         â”‚     â”‚
â”‚  â”‚  âœ“ Files (read/write)              â”‚     â”‚
â”‚  â”‚  âœ“ System (memory/commands)        â”‚     â”‚
â”‚  â”‚  âœ“ Network (API/downloads)         â”‚     â”‚
â”‚  â”‚  âœ“ Search (files/content)          â”‚     â”‚
â”‚  â”‚  âœ“ Utilities (git/compress)        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ’» Your Operating System                â”‚
â”‚  (Files, Commands, Resources)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Normal Query Flow

```
1. User types command
   ğŸ‘¤ "list Python files"
   
2. Client queries LLM
   ğŸ’¬ â†’ ğŸ¤– "Which tool to use?"
   
3. LLM decides tool
   ğŸ¤– â†’ ğŸ’¬ "[USE_TOOL:SEARCH:.:*.py]"
   
4. Client calls MCP server
   ğŸ’¬ â†’ ğŸ”§ {"method": "search_files", ...}
   
5. Server executes tool
   ğŸ”§ â†’ ğŸ’» Real system search
   
6. Server returns results
   ğŸ”§ â†’ ğŸ’¬ {"result": ["main.py", ...]}
   
7. Client sends results to LLM
   ğŸ’¬ â†’ ğŸ¤– "Files found: ..."
   
8. LLM generates natural response
   ğŸ¤– â†’ ğŸ’¬ "Found 12 Python files: ..."
   
9. User sees response
   ğŸ’¬ â†’ ğŸ‘¤ "Found 12 Python files: ..."
```

### Agentic Mode Flow

```
1. User gives complex command
   ğŸ‘¤ "download X and then compress Y"
   
2. Client detects agentic mode
   ğŸ’¬ [Detects keywords "and then"]
   
3. LLM plans steps
   ğŸ’¬ â†’ ğŸ¤– "Break down into steps"
   ğŸ¤– â†’ ğŸ’¬ ["DOWNLOAD:...", "SEARCH:...", "COMPRESS:..."]
   
4. Client executes steps sequentially
   ğŸ’¬ â†’ ğŸ”§ Step 1: DOWNLOAD âœ…
   ğŸ’¬ â†’ ğŸ”§ Step 2: SEARCH âœ…
   ğŸ’¬ â†’ ğŸ”§ Step 3: COMPRESS âœ…
   
5. LLM synthesizes results
   ğŸ’¬ â†’ ğŸ¤– "Summarize everything done"
   ğŸ¤– â†’ ğŸ’¬ "Downloaded, searched and compressed..."
   
6. User sees final summary
   ğŸ’¬ â†’ ğŸ‘¤ "âœ… Task completed: ..."
```

---

## ğŸ“š Additional Resources

### Model Context Protocol (MCP)
- ğŸ“– [MCP Specification](https://spec.modelcontextprotocol.io/)
- ğŸ”— [Anthropic MCP GitHub](https://github.com/anthropics/mcp)

### Recommended Models
- ğŸ¦™ [Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- ğŸŒŸ [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- ğŸš€ [Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)

### llama.cpp
- ğŸ”— [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- ğŸ“– [Documentation](https://github.com/ggerganov/llama.cpp/blob/master/README.md)

---

## ğŸ“ Use Cases

### For Developers
- âœ… Automate repetitive tasks
- âœ… Analyze code and search TODOs
- âœ… Manage Git repositories
- âœ… Generate documentation
- âœ… Monitor system resources

### For System Administrators
- âœ… Automate backups
- âœ… Monitor logs
- âœ… Manage configuration files
- âœ… Search information in logs
- âœ… Compress/decompress files

### For Advanced Users
- âœ… Automatically organize files
- âœ… Download and process web content
- âœ… Search information in documents
- âœ… Automate complex workflows
- âœ… Integrate with external APIs

---

## ğŸ¤ Contributing

Have ideas to improve MCP Local? Contribute!

### New Tool Ideas
- ğŸ“§ Email client
- ğŸ“… Calendar integration
- ğŸ—„ï¸ Database operations
- ğŸ³ Docker integration
- ğŸ“Š Report generation

### How to Contribute
1. Fork the project
2. Create a branch (`git checkout -b feature/new-tool`)
3. Commit your changes (`git commit -am 'Add new tool X'`)
4. Push to the branch (`git push origin feature/new-tool`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is under the MIT license. Use it freely, modify it, and share it.

```
MIT License

Copyright (c) 2025 Gustavo Silva Da Costa

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ™ Acknowledgments

- Anthropic for the Model Context Protocol concept
- llama.cpp community for making it possible to run LLMs locally
- Everyone who contributes to the open source AI ecosystem

---

## ğŸ“ Support

Problems? Questions? Suggestions?

- ğŸ“§ Email: gsilvadacosta0@gmail.com 
- ğŸ†‡ Former Twitter ğŸ˜‚: https://x.com/bibliogalactic

---

<div align="center">

## â­ If you like this project, give it a star on GitHub â­

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                        â•‘
â•‘   Made with â¤ï¸ for the local AI community             â•‘
â•‘                                                        â•‘
â•‘   "Giving hands to AIs, one tool at a time"           â•‘
â•‘                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### ğŸ‘¨â€ğŸ’» Created by

**Gustavo Silva Da Costa** (Eto Demerzel) ğŸ¤«

ğŸš€ *Transforming local AIs into powerful assistants*

</div>

---

**Version:** 1.0.0  
**Last updated:** October 2025  
**Status:** âœ… Production

---
