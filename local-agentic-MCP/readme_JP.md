# 🤖 MCP Local - システムツールを備えたAIチャット

> **11のツールとエージェントモードを備えた完全なModel Context Protocolシステム（ローカルAI用）**

```
╔════════════════════════════════════════════════════════════╗
║                                                            ║
║       ローカルLLMを強力なアシスタントに変換                  ║
║       オペレーティングシステムへのアクセス付き                ║
║                                                            ║
╚════════════════════════════════════════════════════════════╝
```

---

## 📋 目次

- [これは何ですか？](#-これは何ですか)
- [機能](#-機能)
- [要件](#-要件)
- [インストール](#-インストール)
- [基本的な使い方](#-基本的な使い方)
- [エージェントモード](#-エージェントモード)
- [11のツール](#-11のツール)
- [実用例](#-実用例)
- [高度な設定](#-高度な設定)
- [トラブルシューティング](#-トラブルシューティング)
- [アーキテクチャ](#-アーキテクチャ)
- [クレジット](#-クレジット)

---

## 🎯 これは何ですか？

**MCP Local**は、ローカル言語モデル（Mistral、Llamaなど）を**オペレーティングシステムの実際のツール**と接続するシステムです。

### MCPなし：
```
👤 ユーザー：「Pythonファイルをリストして」
🤖 AI：「申し訳ありませんが、ファイルシステムにアクセスできません」
```

### MCPあり：
```
👤 ユーザー：「Pythonファイルをリストして」
🤖 AI：[検索] ✓
       12個のファイルが見つかりました：main.py、utils.py、config.py...
```

**これは、AIにコンピュータと対話する手を与えるようなものです** 🦾

---

## ✨ 機能

### 🔧 11の完全なツール
- ✅ ファイルの読み書き
- ✅ bashコマンドの実行
- ✅ ディレクトリのナビゲーション
- ✅ ファイルとコンテンツの検索
- ✅ HTTP APIのクエリ
- ✅ URLからのファイルダウンロード
- ✅ 圧縮/解凍（zip、tar、tar.gz）
- ✅ Git操作（status、log、diff、branch）
- ✅ システム監視（RAM、CPU、ディスク）
- ✅ コンテンツ検索（grep）

### 🧠 エージェントモード
**スター機能！** AIが複数のアクションを自動的にチェーンできます：

```
👤：「GitHubからREADMEをダウンロードして、すべてのmarkdownファイルを圧縮して」

🤖 [エージェントモード]
   📋 計画：3ステップ
   🔄 ダウンロード中... ✅
   🔄 *.mdを検索中... ✅  
   🔄 圧縮中... ✅
   
   ✅ READMEをダウンロードしました（3.4KB）、5個のmarkdownファイルを見つけ、
      docs.zipに圧縮しました（45KB）
```

### 🔒 組み込みセキュリティ
- ❌ 危険なコマンドをブロック（rm、dd、sudoなど）
- 🛡️ $HOMEまたは/tmpでのみ書き込み可能
- ⏱️ 自動タイムアウト
- 📦 ファイルサイズ制限（10MB）

### 🎨 使いやすいインターフェース
- 💬 インタラクティブチャット
- 📊 デバッグ用の詳細モード
- 🎯 エージェントモードの自動検出
- ⚡ 迅速で明確な応答

---

## 📦 要件

インストール前に、以下があることを確認してください：

### 必須要件
```bash
✅ Python 3.8以上
✅ pip3
✅ GGUFモデル（Mistral、Llamaなど）
✅ llama-cliでコンパイルされたllama.cpp
```

### オプション要件
```bash
🔧 git（Gitツール用）
🔧 curl/wget（macOS/Linuxに含まれています）
```

### オペレーティングシステム
- ✅ macOS（テスト済み）
- ✅ Linux（テスト済み）
- ⚠️ Windows（WSL使用）

---

## 🚀 インストール

### ステップ1：インストーラーのダウンロード

```bash
# オプションA：リポジトリをクローン
git clone https://github.com/your-repo/mcp-local.git
cd mcp-local

# オプションB：スクリプトを直接ダウンロード
curl -O https://your-url/mcp_setup.sh
chmod +x mcp_setup.sh
```

### ステップ2：インストーラーの実行

```bash
./mcp_setup.sh
```

### ステップ3：パスの設定

インストーラーは2つのパスを尋ねます：

```
🎯 初期設定
==========================================

📍 ステップ1/2：llama-cli実行ファイルへのパス
   例：/usr/local/bin/llama-cli
   または：/Users/your-user/llama.cpp/build/bin/llama-cli
   完全なパス：_

📍 ステップ2/2：GGUFモデルへのパス
   例：/Users/your-user/models/mistral-7b-instruct.gguf
   完全なパス：_
```

### ステップ4：自動インストール

スクリプトは自動的に：
1. ✅ Python仮想環境を作成
2. ✅ 依存関係をインストール（flask、psutil、requests）
3. ✅ MCPサーバーを生成（11ツール）
4. ✅ エージェントモード付きクライアントを生成
5. ✅ 設定を保存

```
✅ インストール完了

╔════════════════════════════════════════╗
║     MCP LOCAL - メインメニュー         ║
║     💪 11ツール + エージェント         ║
╚════════════════════════════════════════╝

  1) 💬 チャットを開始（エージェントモード付き）
  2) 🔧 MCPツールを表示（11個）
  3) ⚙️  パスを再設定
  4) 🚪 終了
```

---

## 💬 基本的な使い方

### チャットの開始

```bash
./mcp_setup.sh
# オプション1）チャットを開始を選択
```

### チャットコマンド

```
👤 あなた：_

利用可能なコマンド：
  agentic on/off  → エージェントモードを有効/無効
  verbose on/off  → 詳細ログを表示
  herramientas    → 11のツールをリスト
  salir           → チャットを閉じる
```

### 通常の会話例

```bash
👤 あなた：デスクトップのファイルをリストして

🤖 AI：[リスト] ✓
   デスクトップに23個のアイテムがあります：Documents/、Downloads/、
   image.png、notes.txt...

👤 あなた：空きRAMはどれくらい？

🤖 AI：[メモリ] ✓
   16GB中8.5GBの空きRAMがあります（53%空き）
```

---

## 🧠 エージェントモード

エージェントモードを使用すると、AIが**複数のアクションを自動的にチェーン**でき、1つずつコマンドを出す必要がありません。

### 有効化方法

**オプション1：手動**
```bash
👤 あなた：agentic on
🤖 エージェントモード：有効
```

**オプション2：自動**（これらのキーワードを検出）
- `そして`
- `その後`
- `そして圧縮`
- `そして検索`
- `すべて完了`
- `すべて実行`
- `自動`

### 完全な例

#### エージェントモードなし（3つの別々のコマンド）：
```bash
👤：READMEをダウンロード
🤖：✓

👤：すべてのmarkdownファイルを検索
🤖：✓

👤：ファイルを圧縮
🤖：✓
```

#### エージェントモードあり（1つのコマンド）：
```bash
👤：GitHubからREADMEをダウンロードして、そしてすべてのmarkdownファイルを圧縮して

🤖 [エージェントモード有効]
📋 計画：3ステップ

🔄 ステップ1/3：ダウンロード:https://raw.githubusercontent.com/...
   ✅ ダウンロード

🔄 ステップ2/3：検索:~/Desktop:*.md
   ✅ 検索

🔄 ステップ3/3：圧縮:compress:~/Desktop:~/Desktop/docs.zip
   ✅ 圧縮

🔄 結果を統合中...

✅ タスク完了

🤖 READMEをダウンロードしました（3456バイト）、デスクトップで5個の
   markdownファイルを見つけ、docs.zipに圧縮しました
   （合計45KB）。完了しました！
```

### 詳細モード（デバッグ）

内部プロセスを確認するには：

```bash
👤 あなた：verbose on
📊 詳細モード：有効

👤 あなた：XをダウンロードしてYを圧縮

🧠 ステップを計画中...
📋 計画されたステップ：["ダウンロード:...", "検索:...", "圧縮:..."]
🔍 実行中：ダウンロード:https://...
   ✅ ダウンロード
🔍 実行中：検索:~/Desktop:*.md
   ✅ 検索
...
```

---

## 🛠️ 11のツール

### 1. 📖 ファイルを読む
```bash
👤：README.mdファイルを読んで
🤖：[読取] ✓
   ファイルには...に関するドキュメントが含まれています
```
- 📦 最大：64KB
- 🔒 テキストファイルのみ

### 2. ✍️ ファイルに書く
```bash
👤：「Hello World」を含むtest.txtファイルを作成して
🤖：[書込] ✓（11バイト）
   ファイルが~/test.txtに作成されました
```
- 📦 最大：10MB
- 🔒 $HOMEまたは/tmpのみ
- 🔀 モード：`w`（上書き）または`a`（追加）

### 3. 📁 ディレクトリをリスト
```bash
👤：Downloadsフォルダには何がある？
🤖：[リスト] ✓
   45個のアイテム：documents/、images/、video.mp4...
```
- 📊 表示：名前、タイプ、サイズ、日付
- 📦 制限：100アイテム

### 4. 🔍 ファイルを検索
```bash
👤：すべてのPythonファイルを見つけて
🤖：[検索] ✓
   12個のファイルが見つかりました：main.py、utils.py...
```
- 🌲 再帰検索
- 🎯 Globパターン：`*.py`、`test*.txt`など
- 📦 制限：50ファイル

### 5. 🔎 コンテンツを検索（Grep）
```bash
👤：Pythonファイルで「TODO」を検索して
🤖：[GREP] ✓（8個の一致）
   main.py:42: # TODO: 検証を実装
   utils.py:15: # TODO: アルゴリズムを最適化
```
- 📄 1MB未満のファイルのみ
- 🎯 大文字小文字を区別しない正規表現
- 📦 制限：50個の一致

### 6. ⚡ コマンドを実行
```bash
👤：ls -laを実行して
🤖：[コマンド] ✓
   total 256
   drwxr-xr-x  15 user  staff   480 Oct 10 10:30 .
   ...
```
- ❌ **ブロック済み**：rm、dd、sudo、su、mkfs
- ⏱️ タイムアウト：10秒
- 📦 出力：最大4KB

### 7. 💾 メモリをクエリ
```bash
👤：利用可能なリソースはどれくらい？
🤖：[メモリ] ✓
   RAM：16GB中8.5GB空き
   CPU：35%使用（8コア）
   ディスク：500GB中245GB空き
```

### 8. 📥 ファイルをダウンロード
```bash
👤：https://example.com/file.pdfをダウンロードして
🤖：[ダウンロード] ✓（2.5MB）
   ファイルが~/Downloads/file.pdfに保存されました
```
- 🌐 http://とhttps://のみ
- 📦 制限：10MB
- ⏱️ タイムアウト：30秒

### 9. 🗜️ 圧縮/解凍
```bash
👤：documentsフォルダを圧縮して
🤖：[圧縮] ✓
   フォルダがdocuments.zipに圧縮されました（12MB）
```
- 📦 形式：zip、tar、tar.gz
- 🔄 操作：圧縮、解凍

### 10. 🔧 Git操作
```bash
👤：gitステータスを表示して
🤖：[GIT] ✓
   On branch main
   Changes not staged for commit:
     modified: src/main.py
```
- ✅ 許可：status、log、diff、branch、remote
- ❌ **ブロック済み**：commit、push、pull（読み取り専用）

### 11. 🌐 APIをクエリ
```bash
👤：example.com APIをクエリして
🤖：[API] ✓（200）
   ステータス：OK
   レスポンス：{"data": "example"}
```
- 📡 メソッド：GET、POST、PUT、DELETE、PATCH
- ⏱️ タイムアウト：10秒
- 📦 レスポンス：最大4KB

---

## 💡 実用例

### 例1：プロジェクト分析
```bash
👤：agentic on

👤：すべてのPythonファイルをリストして、保留中のTODOを検索して

🤖 [エージェントモード]
   ステップ1：*.pyを検索中... ✅
   ステップ2：'TODO'を検索中... ✅
   
   12個のPythonファイルが見つかり、8個の保留中のTODOがあります：
   - main.py：3個のTODO
   - utils.py：2個のTODO
   - config.py：3個のTODO
```

### 例2：自動バックアップ
```bash
👤：すべての重要なファイルを検索して、バックアップに圧縮して

🤖 [エージェントモード]
   ステップ1：ファイルを検索中... ✅
   ステップ2：圧縮中... ✅
   
   backup.zipを作成しました、45個のファイル（合計234MB）
```

### 例3：システム監視
```bash
👤：gitステータスとシステムリソースを表示して

🤖 [エージェントモード]
   ステップ1：Gitステータス... ✅
   ステップ2：リソースをクエリ中... ✅
   
   Git：mainブランチで3個の変更されたファイル
   システム：RAM 45%空き、CPU 25%、ディスク50%空き
```

### 例4：完全なワークフロー
```bash
👤：GitHubからREADMEをダウンロードして、デスクトップで検索し、
    見つけたすべてのmarkdownファイルを圧縮して

🤖 [エージェントモード]
   📋 計画：3ステップ
   
   ステップ1：GitHubからダウンロード中... ✅（3.4KB）
   ステップ2：デスクトップで*.mdを検索中... ✅（5ファイル）
   ステップ3：ファイルを圧縮中... ✅（45KB）
   
   ✅ READMEをダウンロードし、5個のmarkdownファイルを見つけ、
      docs.zipに圧縮しました。すべてデスクトップにあります。
```

---

## ⚙️ 高度な設定

### モデルまたはllama-cliパスの変更

```bash
./mcp_setup.sh
# オプション3）パスを再設定を選択
```

### 手動で設定を編集

```bash
nano ~/.mcp_local/config.env
```

```bash
# MCP Local設定
LLAMA_CLI="/path/to/your/llama-cli"
MODELO_GGUF="/path/to/your/model.gguf"
```

### 環境変数

```bash
# MCPサーバーのデバッグを有効化
export MCP_DEBUG=1

# 実行
./mcp_setup.sh
```

### ファイル構造

```
~/.mcp_local/
├── config.env           # あなたの設定
├── venv/                # Python環境
├── mcp_server.py        # 11ツール搭載のサーバー
└── chat_mcp.py          # エージェントモード搭載のクライアント
```

---

## 🔧 トラブルシューティング

### 問題：「llama-cliが見つかりません」

**解決策：**
```bash
# llama.cppがコンパイルされていることを確認
cd ~/llama.cpp
cmake -B build
cmake --build build

# パスを確認
ls ~/llama.cpp/build/bin/llama-cli

# MCPを再設定
./mcp_setup.sh
# オプション3）パスを再設定
```

### 問題：「モデルが見つかりません」

**解決策：**
```bash
# モデルが存在することを確認
ls ~/path/to/your/model.gguf

# モデルがない場合は、ダウンロード
# 例：Mistral 7B
wget https://huggingface.co/...model.gguf

# 再設定
./mcp_setup.sh
# オプション3）パスを再設定
```

### 問題：「Python依存関係のインストールエラー」

**解決策：**
```bash
# Pythonを確認
python3 --version  # 3.8+である必要があります

# 仮想環境をクリーン
rm -rf ~/.mcp_local/venv

# 再インストール
./mcp_setup.sh
```

### 問題：「エージェントモードがうまく機能しない」

**解決策：**
```bash
# 詳細モードを使用して何が起こっているか確認
👤：verbose on
👤：問題のあるコマンド

# エージェントモードはモデルの品質に依存します
# 推奨モデル：
# - Mistral 7B Instruct（最小）
# - Llama 3 8B Instruct（より良い）
# - Mixtral 8x7B（最適）
```

### 問題：「クエリタイムアウト」

**解決策：**
```bash
# モデルが非常に遅い場合は、タイムアウトを増やす
# ~/.mcp_local/chat_mcp.pyを編集

# 約40行目：
IA_CMD = [
    config.get('LLAMA_CLI', 'llama-cli'),
    "--model", config.get('MODELO_GGUF', ''),
    "--n-predict", "512",
    "--temp", "0.7",
    "--ctx-size", "4096"
]

# GPUが利用可能な場合は追加：
# "--n-gpu-layers", "35"
```

### 問題：「セキュリティ上の理由でコマンドがブロックされました」

**解決策：**
これは意図的です。危険なコマンドはブロックされています：
- ❌ `rm -rf`
- ❌ `dd`
- ❌ `sudo`
- ❌ `su`

特権コマンドを実行する必要がある場合は、MCP外で手動で実行してください。

---

## 🏗️ アーキテクチャ

```
┌─────────────────────────────────────────────┐
│           👤 ユーザー（あなた）              │
└─────────────────┬───────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────┐
│      💬 チャットクライアント（chat_mcp.py）  │
│  ┌────────────────────────────────────┐     │
│  │  🧠 エージェントモード              │     │
│  │  - ステップ計画                    │     │
│  │  - 順次実行                        │     │
│  │  - 結果の統合                      │     │
│  └────────────────────────────────────┘     │
└─────────────────┬───────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────┐
│        🤖 ローカルLLMモデル                 │
│     （Mistral、Llama、Mixtralなど）        │
└─────────────────┬───────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────┐
│   🔧 MCPサーバー（mcp_server.py）           │
│  ┌────────────────────────────────────┐     │
│  │  11のツール：                      │     │
│  │  ✓ ファイル（読/書）               │     │
│  │  ✓ システム（メモリ/コマンド）      │     │
│  │  ✓ ネットワーク（API/ダウンロード） │     │
│  │  ✓ 検索（ファイル/コンテンツ）      │     │
│  │  ✓ ユーティリティ（git/圧縮）       │     │
│  └────────────────────────────────────┘     │
└─────────────────┬───────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────┐
│     💻 あなたのオペレーティングシステム      │
│  （ファイル、コマンド、リソース）            │
└─────────────────────────────────────────────┘
```

### 通常のクエリフロー

```
1. ユーザーがコマンドを入力
   👤 "Pythonファイルをリスト"
   
2. クライアントがLLMをクエリ
   💬 → 🤖 "どのツールを使用？"
   
3. LLMがツールを決定
   🤖 → 💬 "[ツール使用:検索:.:*.py]"
   
4. クライアントがMCPサーバーを呼び出し
   💬 → 🔧 {"method": "search_files", ...}
   
5. サーバーがツールを実行
   🔧 → 💻 実際のシステム検索
   
6. サーバーが結果を返す
   🔧 → 💬 {"result": ["main.py", ...]}
   
7. クライアントが結果をLLMに送信
   💬 → 🤖 "見つかったファイル：..."
   
8. LLMが自然な応答を生成
   🤖 → 💬 "12個のPythonファイルが見つかりました：..."
   
9. ユーザーが応答を見る
   💬 → 👤 "12個のPythonファイルが見つかりました：..."
```

### エージェントモードフロー

```
1. ユーザーが複雑なコマンドを入力
   👤 "Xをダウンロードして、そしてYを圧縮"
   
2. クライアントがエージェントモードを検出
   💬 [キーワード「そして」を検出]
   
3. LLMがステップを計画
   💬 → 🤖 "ステップに分解"
   🤖 → 💬 ["ダウンロード:...", "検索:...", "圧縮:..."]
   
4. クライアントがステップを順次実行
   💬 → 🔧 ステップ1：ダウンロード ✅
   💬 → 🔧 ステップ2：検索 ✅
   💬 → 🔧 ステップ3：圧縮 ✅
   
5. LLMが結果を統合
   💬 → 🤖 "実行されたすべてをまとめる"
   🤖 → 💬 "ダウンロード、検索、圧縮しました..."
   
6. ユーザーが最終サマリーを見る
   💬 → 👤 "✅ タスク完了：..."
```

---

## 📚 追加リソース

### Model Context Protocol（MCP）
- 📖 [MCP仕様](https://spec.modelcontextprotocol.io/)
- 🔗 [Anthropic MCP GitHub](https://github.com/anthropics/mcp)

### 推奨モデル
- 🦙 [Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- 🌟 [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- 🚀 [Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)

### llama.cpp
- 🔗 [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- 📖 [ドキュメント](https://github.com/ggerganov/llama.cpp/blob/master/README.md)

---

## 🎓 ユースケース

### 開発者向け
- ✅ 反復タスクの自動化
- ✅ コード分析とTODO検索
- ✅ Gitリポジトリの管理
- ✅ ドキュメントの生成
- ✅ システムリソースの監視

### システム管理者向け
- ✅ バックアップの自動化
- ✅ ログの監視
- ✅ 設定ファイルの管理
- ✅ ログ内の情報検索
- ✅ ファイルの圧縮/解凍

### 上級ユーザー向け
- ✅ ファイルの自動整理
- ✅ Webコンテンツのダウンロードと処理
- ✅ ドキュメント内の情報検索
- ✅ 複雑なワークフローの自動化
- ✅ 外部APIとの統合

---

## 🤝 貢献

MCP Localを改善するアイデアはありますか？貢献してください！

### 新しいツールのアイデア
- 📧 メールクライアント
- 📅 カレンダー統合
- 🗄️ データベース操作
- 🐳 Docker統合
- 📊 レポート生成

### 貢献方法
1. プロジェクトをFork
2. ブランチを作成（`git checkout -b feature/new-tool`）
3. 変更をコミット（`git commit -am 'Add new tool X'`）
4. ブランチにプッシュ（`git push origin feature/new-tool`）
5. プルリクエストを開く

---

## 📄 ライセンス

このプロジェクトはMITライセンスの下にあります。自由に使用、変更、共有してください。

```
MITライセンス

Copyright (c) 2025 Gustavo Silva Da Costa

本ソフトウェアおよび関連文書ファイル（以下「ソフトウェア」）のコピーを
取得した人は誰でも、ソフトウェアを使用、コピー、変更、マージ、公開、配布、
サブライセンス、および/または販売する権利を含む、ソフトウェアを制限なく
扱う権利が無償で付与されます。ただし、次の条件に従うものとします：

上記の著作権表示およびこの許可表示は、ソフトウェアのすべてのコピー
または実質的な部分に含まれるものとします。

ソフトウェアは「現状のまま」提供され、明示的または黙示的を問わず、
商品性、特定目的への適合性、および非侵害の保証を含むがこれらに限定されない、
いかなる種類の保証もありません。いかなる場合においても、著者または著作権
所有者は、契約行為、不法行為、またはその他の行為において、ソフトウェアまたは
ソフトウェアの使用またはその他の取引から生じる、またはそれに関連する
いかなる請求、損害、またはその他の責任についても責任を負いません。
```

---

## 🙏 謝辞

- Model Context Protocolの概念を提供したAnthropic
- ローカルでLLMを実行可能にしたllama.cppコミュニティ
- オープンソースAIエコシステムに貢献するすべての人

---

## 📞 サポート

問題がありますか？質問がありますか？提案がありますか？

- 📧 メール：gsilvadacosta0@gmail.com 
- 🆇 旧Twitter 😂：https://x.com/bibliogalactic

---

<div align="center">

## ⭐ このプロジェクトが気に入ったら、GitHubでスターをください ⭐

```
╔════════════════════════════════════════════════════════╗
║                                                        ║
║   ローカルAIコミュニティのために❤️を込めて作成         ║
║                                                        ║
║   「AIに手を与える、一度に一つのツールずつ」            ║
║                                                        ║
╚════════════════════════════════════════════════════════╝
```

### 👨‍💻 作成者

**Gustavo Silva Da Costa** (Eto Demerzel) 🤫

🚀 *ローカルAIを強力なアシスタントに変換*

</div>

---

**バージョン：** 1.0.0  
**最終更新：** 2025年10月  
**ステータス：** ✅ プロダクション

---
